{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# if not hasattr(torch._dynamo, 'external_utils'):\n",
    "#     import types\n",
    "#     torch._dynamo.external_utils = types.ModuleType('external_utils')\n",
    "#     torch._dynamo.external_utils.is_compiling = lambda: True\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_cbr import CBRLanguageModel\n",
    "from grid_search import WordTokenizer\n",
    "import torch\n",
    "from clean_RTs import prepare_data_correct, fit_simple_lmer, compute_delta_loglik_correct, run_paper_analysis_correct\n",
    "from grid_search import WordTokenizer  # assuming your tokenizer class is here\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.read_csv('/scratch2/mrenaudin/Hard-CBR-RNN/all_stories.tok', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_cbr(checkpoint_path):\n",
    "    \"\"\"Load the trained Lightning model\"\"\"\n",
    "    model = CBRLanguageModel.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entire_transformer import SimpleTransformerLM\n",
    "def load_trained_transformer(checkpoint_path):\n",
    "    model = SimpleTransformerLM.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_with_chunking_cbr(lightning_model, stories_df, tokenizer, chunk_size=64):\n",
    "    \"\"\"\n",
    "    Compute surprisal using the same chunking as training (seq_len=35)\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning CBRLanguageModel\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (35)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    \n",
    "    # Extract the actual CBR_RNN model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Process in chunks of 35 tokens (matching training)\n",
    "        story_surprisals = [float('nan')] * len(words)  # Initialize with NaN\n",
    "        \n",
    "        for start_idx in range(0, len(token_ids), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "            chunk_ids = token_ids[start_idx:end_idx]\n",
    "            \n",
    "            if len(chunk_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensor [chunk_len, batch_size=1]\n",
    "            input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "            \n",
    "            # Initialize cache for this chunk\n",
    "            initial_cache = model.init_cache(input_tensor)\n",
    "            \n",
    "            # Forward pass (same as training)\n",
    "            with torch.no_grad():\n",
    "                # Use same parameters as training\n",
    "                forward_kwargs = {\n",
    "                    'observation': input_tensor,\n",
    "                    'initial_cache': initial_cache\n",
    "                }\n",
    "                \n",
    "                # Add Gumbel parameters if model was trained with them\n",
    "                if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                    forward_kwargs.update({\n",
    "                        'temperature': 0.1,  # Use fixed temp for inference\n",
    "                        'use_gumbel': True   # Don't use Gumbel for inference\n",
    "                    })\n",
    "                \n",
    "                logits, states = model(**forward_kwargs)\n",
    "                # logits shape: [chunk_len, 1, vocab_size]\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Compute surprisal for each position in chunk\n",
    "                for i in range(1, len(chunk_ids)):  # Skip first token (no context)\n",
    "                    target_token_id = chunk_ids[i]\n",
    "                    # Use prediction from step i-1 for token at step i\n",
    "                    log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                    surprisal = -log_prob\n",
    "                    \n",
    "                    # Map back to story position\n",
    "                    story_pos = start_idx + i\n",
    "                    if story_pos < len(story_surprisals):\n",
    "                        story_surprisals[story_pos] = surprisal\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_with_chunking_transformer(lightning_model, stories_df, tokenizer, chunk_size=64):\n",
    "    \"\"\"\n",
    "    Compute surprisal using the same chunking as training for SimpleTransformer model\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning SimpleTransformerLM\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (64)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    lightning_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Extract the actual SimpleTransformer model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Process in chunks matching training\n",
    "        story_surprisals = [float('nan')] * len(words)  # Initialize with NaN\n",
    "        \n",
    "        for start_idx in range(0, len(token_ids), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "            chunk_ids = token_ids[start_idx:end_idx]\n",
    "            \n",
    "            if len(chunk_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensor [seq_len, batch_size=1] (SimpleTransformer expects this format)\n",
    "            input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                # Prepare forward arguments\n",
    "                forward_kwargs = {\n",
    "                    'src': input_tensor\n",
    "                }\n",
    "                \n",
    "                # Add Gumbel parameters if model was trained with them\n",
    "                if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                    forward_kwargs.update({\n",
    "                        'temperature': 0.5,  # Use moderate temp for inference\n",
    "                        'use_gumbel': False,  # Don't use Gumbel sampling for surprisal\n",
    "                        'hard': False\n",
    "                    })\n",
    "                \n",
    "                # Get logits: [seq_len, batch_size=1, vocab_size]\n",
    "                logits = model(**forward_kwargs)\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Compute surprisal for each position in chunk\n",
    "                for i in range(1, len(chunk_ids)):  # Skip first token (no previous context)\n",
    "                    target_token_id = chunk_ids[i]\n",
    "                    \n",
    "                    # Use prediction from step i-1 for token at step i\n",
    "                    # logits[i-1, 0, :] contains predictions for position i\n",
    "                    log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                    surprisal = -log_prob  # Surprisal = -log(probability)\n",
    "                    \n",
    "                    # Map back to story position\n",
    "                    story_pos = start_idx + i\n",
    "                    if story_pos < len(story_surprisals):\n",
    "                        story_surprisals[story_pos] = surprisal\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer.load('tokenizer.json')\n",
    "checkpoints_cbr_unfinished = {\n",
    "    '128_1_false_19':'/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_000/lightning_logs/version_1354621/checkpoints/epoch=19-step=123800.ckpt',\n",
    "    '512_1_false_15' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_001/lightning_logs/version_1354622/checkpoints/epoch=15-step=99040.ckpt',\n",
    "    '128_8_false_20' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_002/lightning_logs/version_1354623/checkpoints/epoch=20-step=129990.ckpt',\n",
    "    '512_8_false_16' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_003/lightning_logs/version_1354624/checkpoints/epoch=16-step=105230.ckpt',\n",
    "    '128_1_true_17' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_004/lightning_logs/version_1354625/checkpoints/epoch=17-step=111420.ckpt',\n",
    "    '512_1_true_15' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_005/lightning_logs/version_1354626/checkpoints/epoch=15-step=99040.ckpt',\n",
    "    '128_1_true_19' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_006/lightning_logs/version_1354627/checkpoints/epoch=19-step=123800.ckpt',\n",
    "    '512_1_true_15' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_cbr_007/lightning_logs/version_1354619/checkpoints/epoch=15-step=99040.ckpt'   \n",
    "}\n",
    "\n",
    "# Placeholder dictionary to store ΔLogLik\n",
    "delta_loglik_dict = {}\n",
    "\n",
    "# Load your reading time and surprisal data\n",
    "# (You need to have these prepared for your analysis)\n",
    "rt_data = pd.read_csv('processed_RTs.tsv', sep='\\t')\n",
    "\n",
    "# Loop over checkpoints\n",
    "for name, ckpt_path in checkpoints_cbr_unfinished.items():\n",
    "    print(f\"\\nRunning analysis for checkpoint: {name}\")\n",
    "    model = load_trained_cbr(ckpt_path)\n",
    "    \n",
    "    surprisal_data = compute_surprisal_with_chunking(model, stories, tokenizer=tokenizer, chunk_size=64)\n",
    "    \n",
    "    # Run ΔLogLik analysis\n",
    "    results = run_paper_analysis_correct(rt_data, surprisal_data)\n",
    "    \n",
    "    # Store ΔLogLik\n",
    "    delta_loglik_dict[name] = results['delta_loglik']\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== ΔLogLik for all checkpoints ===\")\n",
    "for name, delta in delta_loglik_dict.items():\n",
    "    print(f\"{name}: {delta:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer.load('tokenizer.json')\n",
    "checkpoints_transformer_unfinished = {\n",
    "    '128_1_false':'/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_000/lightning_logs/version_1356201/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    '512_1_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_001/lightning_logs/version_1356202/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    '128_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_002/lightning_logs/version_1356203/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    '512_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_003/lightning_logs/version_1356204/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    '128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_004/lightning_logs/version_1356205/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    '512_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_005/lightning_logs/version_1356206/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    '128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_006/lightning_logs/version_1356207/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    '512_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_007/lightning_logs/version_1356197/checkpoints/epoch=49-step=309500.ckpt'   \n",
    "}\n",
    "\n",
    "# Placeholder dictionary to store ΔLogLik\n",
    "delta_loglik_dict = {}\n",
    "\n",
    "# Load your reading time and surprisal data\n",
    "# (You need to have these prepared for your analysis)\n",
    "rt_data = pd.read_csv('processed_RTs.tsv', sep='\\t')\n",
    "\n",
    "# Loop over checkpoints\n",
    "for name, ckpt_path in checkpoints_transformer_unfinished.items():\n",
    "    print(f\"\\nRunning analysis for checkpoint: {name}\")\n",
    "    model = load_trained_transformer(ckpt_path)\n",
    "    \n",
    "    surprisal_data = compute_surprisal_with_chunking_transformer(model, stories, tokenizer=tokenizer)\n",
    "    \n",
    "    # Run ΔLogLik analysis\n",
    "    results = run_paper_analysis_correct(rt_data, surprisal_data)\n",
    "    \n",
    "    # Store ΔLogLik\n",
    "    delta_loglik_dict[name] = results['delta_loglik']\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== ΔLogLik for all checkpoints ===\")\n",
    "for name, delta in delta_loglik_dict.items():\n",
    "    print(f\"{name}: {delta:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaps3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
