{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.read_csv('/scratch2/mrenaudin/Hard-CBR-RNN/all_stories.tok', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict surprisal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from original_cbr import CBR_RNN\n",
    "checkpoint = '/scratch2/mrenaudin/Hard-CBR-RNN/job_007/lightning_logs/version_1198526/checkpoints/epoch=49-step=565950.ckpt'\n",
    "hparams_path = '/scratch2/mrenaudin/Hard-CBR-RNN/job_007/lightning_logs/version_1198526/hparams.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "with open(hparams_path, 'r') as f:\n",
    "            hparams = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = hparams['vocab_size']+1  # vocabulary size\n",
    "ninp = hparams['ninp']      # embedding dimension\n",
    "nhid = hparams['nhid']      # hidden dimension\n",
    "nlayers = hparams.get('nlayers', 1)  # number of layers\n",
    "nheads = hparams.get('nheads', 1)    # number of attention heads\n",
    "dropout = hparams.get('dropout', 0.5) # dropout rate\n",
    "\n",
    "# Initialize model\n",
    "model = CBR_RNN(\n",
    "    ntoken=ntoken,\n",
    "    ninp=ninp, \n",
    "    nhid=nhid,\n",
    "    nlayers=nlayers,\n",
    "    nheads=nheads,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint, map_location='cpu')\n",
    "\n",
    "# Extract state_dict (might be nested in Lightning checkpoint)\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    # Remove Lightning module prefix if present\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from original_cbr_lightning import CBRLanguageModel, WordTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '/scratch2/mrenaudin/Hard-CBR-RNN/job_007/lightning_logs/version_1198526/checkpoints/epoch=49-step=565950.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path):\n",
    "    \"\"\"Load the trained Lightning model\"\"\"\n",
    "    model = CBRLanguageModel.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_with_chunking(lightning_model, stories_df, tokenizer, chunk_size=35):\n",
    "    \"\"\"\n",
    "    Compute surprisal using the same chunking as training (seq_len=35)\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning CBRLanguageModel\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (35)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    \n",
    "    # Extract the actual CBR_RNN model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer['word2idx'].get(word, 0)#tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Process in chunks of 35 tokens (matching training)\n",
    "        story_surprisals = [float('nan')] * len(words)  # Initialize with NaN\n",
    "        \n",
    "        for start_idx in range(0, len(token_ids), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "            chunk_ids = token_ids[start_idx:end_idx]\n",
    "            \n",
    "            if len(chunk_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensor [chunk_len, batch_size=1]\n",
    "            input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "            \n",
    "            # Initialize cache for this chunk\n",
    "            initial_cache = model.init_cache(input_tensor)\n",
    "            \n",
    "            # Forward pass (same as training)\n",
    "            with torch.no_grad():\n",
    "                # Use same parameters as training\n",
    "                forward_kwargs = {\n",
    "                    'observation': input_tensor,\n",
    "                    'initial_cache': initial_cache\n",
    "                }\n",
    "                \n",
    "                # Add Gumbel parameters if model was trained with them\n",
    "                if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                    forward_kwargs.update({\n",
    "                        'temperature': 1.0,  # Use fixed temp for inference\n",
    "                        'use_gumbel': False   # Don't use Gumbel for inference\n",
    "                    })\n",
    "                \n",
    "                logits, states = model(**forward_kwargs)\n",
    "                # logits shape: [chunk_len, 1, vocab_size]\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Compute surprisal for each position in chunk\n",
    "                for i in range(1, len(chunk_ids)):  # Skip first token (no context)\n",
    "                    target_token_id = chunk_ids[i]\n",
    "                    # Use prediction from step i-1 for token at step i\n",
    "                    log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                    surprisal = -log_prob\n",
    "                    \n",
    "                    # Map back to story position\n",
    "                    story_pos = start_idx + i\n",
    "                    if story_pos < len(story_surprisals):\n",
    "                        story_surprisals[story_pos] = surprisal\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "            tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisal = compute_surprisal_with_chunking(model, stories, tokenizer, chunk_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for reading time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data = pd.read_csv('/scratch2/mrenaudin/Hard-CBR-RNN/processed_RTs.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rt_data.merge(surprisal, on=['item', 'zone'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_RT'] = np.log(data['RT'])                    # y variable\n",
    "data['word_length'] = data['word_x'].str.len()           # create length from word\n",
    "\n",
    "# Clean grouping variable - convert to sequential integers\n",
    "data['subject'] = pd.Categorical(data['WorkerId']).codes\n",
    "\n",
    "# Standardize predictors  \n",
    "data['surprisal_z'] = (data['surprisal'] - data['surprisal'].mean()) / data['surprisal'].std()\n",
    "data['length_z'] = (data['word_length'] - data['word_length'].mean()) / data['word_length'].std()\n",
    "\n",
    "# Reset index to avoid indexing issues\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data prepared: {len(data)} observations\")\n",
    "print(f\"Number of subjects: {data['subject'].nunique()}\")\n",
    "print(f\"Mean word length: {data['word_length'].mean():.1f} characters\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "\n",
    "y = data['log_RT'].values\n",
    "X = data[['surprisal_z', 'length_z']].values\n",
    "X = np.column_stack([np.ones(len(X)), X])  # Add intercept\n",
    "groups = data['subject'].values\n",
    "\n",
    "# Remove any NaN values\n",
    "mask = ~(np.isnan(y) | np.isnan(X).any(axis=1) | np.isnan(groups))\n",
    "y = y[mask]\n",
    "X = X[mask]\n",
    "groups = groups[mask]\n",
    "\n",
    "print(f\"Clean data: {len(y)} observations\")\n",
    "\n",
    "model = MixedLM(y, X, groups=groups)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(fitted_model):\n",
    "    \"\"\"Print key results\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"READING TIME PREDICTION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check if it's a fitted model\n",
    "    if hasattr(fitted_model, 'summary'):\n",
    "        print(fitted_model.summary().tables[1])  # Coefficients table only\n",
    "        \n",
    "        # Get coefficients using array indexing\n",
    "        surprisal_coef = fitted_model.params[1]  # x1 = surprisal\n",
    "        surprisal_p = fitted_model.pvalues[1]\n",
    "        \n",
    "        length_coef = fitted_model.params[2]     # x2 = word length  \n",
    "        length_p = fitted_model.pvalues[2]\n",
    "            \n",
    "        print(f\"\\nSurprisal coefficient (x1): {surprisal_coef:.4f}\")\n",
    "        print(f\"P-value: {surprisal_p:.4f}\")\n",
    "        print(f\"Significant: {'Yes' if surprisal_p < 0.05 else 'No'}\")\n",
    "        \n",
    "        print(f\"\\nWord length coefficient (x2): {length_coef:.4f}\")\n",
    "        print(f\"P-value: {length_p:.4f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if surprisal_coef > 0:\n",
    "            print(\"✓ Higher surprisal → Longer reading times (expected)\")\n",
    "        else:\n",
    "            print(\"⚠ Higher surprisal → Shorter reading times (unexpected)\")\n",
    "            print(\"  This suggests an issue with surprisal computation or model\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Model not properly fitted!\")\n",
    "        print(f\"Model type: {type(fitted_model)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaps3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
