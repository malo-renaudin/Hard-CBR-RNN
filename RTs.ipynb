{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# if not hasattr(torch._dynamo, 'external_utils'):\n",
    "#     import types\n",
    "#     torch._dynamo.external_utils = types.ModuleType('external_utils')\n",
    "#     torch._dynamo.external_utils.is_compiling = lambda: True\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_cbr import CBRLanguageModel\n",
    "from grid_search import WordTokenizer\n",
    "import torch\n",
    "from clean_RTs import prepare_data_correct, fit_simple_lmer, compute_delta_loglik_correct, run_paper_analysis_correct\n",
    "from grid_search import WordTokenizer  # assuming your tokenizer class is here\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.read_csv('/scratch2/mrenaudin/Hard-CBR-RNN/all_stories.tok', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_cbr(checkpoint_path):\n",
    "    \"\"\"Load the trained Lightning model\"\"\"\n",
    "    model = CBRLanguageModel.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entire_transformer import SimpleTransformerLM\n",
    "def load_trained_transformer(checkpoint_path):\n",
    "    model = SimpleTransformerLM.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_with_chunking_cbr(lightning_model, stories_df, tokenizer, chunk_size=64):\n",
    "    \"\"\"\n",
    "    Compute surprisal using the same chunking as training (seq_len=35)\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning CBRLanguageModel\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (35)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    \n",
    "    # Extract the actual CBR_RNN model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Process in chunks of 35 tokens (matching training)\n",
    "        story_surprisals = [float('nan')] * len(words)  # Initialize with NaN\n",
    "        \n",
    "        for start_idx in range(0, len(token_ids), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "            chunk_ids = token_ids[start_idx:end_idx]\n",
    "            \n",
    "            if len(chunk_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensor [chunk_len, batch_size=1]\n",
    "            input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "            \n",
    "            # Initialize cache for this chunk\n",
    "            initial_cache = model.init_cache(input_tensor)\n",
    "            \n",
    "            # Forward pass (same as training)\n",
    "            with torch.no_grad():\n",
    "                # Use same parameters as training\n",
    "                forward_kwargs = {\n",
    "                    'observation': input_tensor,\n",
    "                    'initial_cache': initial_cache\n",
    "                }\n",
    "                \n",
    "                # Add Gumbel parameters if model was trained with them\n",
    "                if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                    forward_kwargs.update({\n",
    "                        'temperature':0.1,  # Use fixed temp for inference\n",
    "                        'use_gumbel':    # Don't use Gumbel for inference\n",
    "                    })\n",
    "                \n",
    "                logits, states = model(**forward_kwargs)\n",
    "                # logits shape: [chunk_len, 1, vocab_size]\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)/math.log(2)\n",
    "                \n",
    "                # Compute surprisal for each position in chunk\n",
    "                for i in range(1, len(chunk_ids)):  # Skip first token (no context)\n",
    "                    target_token_id = chunk_ids[i]\n",
    "                    # Use prediction from step i-1 for token at step i\n",
    "                    log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                    surprisal = -log_prob/math.log(2)\n",
    "                    \n",
    "                    # Map back to story position\n",
    "                    story_pos = start_idx + i\n",
    "                    if story_pos < len(story_surprisals):\n",
    "                        story_surprisals[story_pos] = surprisal\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_cbr(lightning_model, stories_df, tokenizer, max_seq_len=1024):\n",
    "    \"\"\"\n",
    "    Compute surprisal by processing each story as a continuous sequence\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning CBRLanguageModel\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        max_seq_len: Maximum sequence length to process at once\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    \n",
    "    # Extract the actual CBR_RNN model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(token_ids) > max_seq_len:\n",
    "            token_ids = token_ids[:max_seq_len]\n",
    "            print(f\"Warning: Story {story_id} truncated to {max_seq_len} tokens\")\n",
    "        \n",
    "        if len(token_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "            continue\n",
    "        \n",
    "        # Convert to tensor [seq_len, batch_size=1]\n",
    "        input_tensor = torch.tensor(token_ids).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Initialize cache once for the entire story\n",
    "        initial_cache = model.init_cache(input_tensor)\n",
    "        \n",
    "        # Forward pass for entire story\n",
    "        with torch.no_grad():\n",
    "            forward_kwargs = {\n",
    "                'observation': input_tensor,\n",
    "                'initial_cache': initial_cache\n",
    "            }\n",
    "            \n",
    "            # Add Gumbel parameters if model was trained with them\n",
    "            if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                forward_kwargs.update({\n",
    "                    'temperature': 0.1,  # Use fixed temp for inference\n",
    "                    'use_gumbel': False  # Don't use Gumbel for inference\n",
    "                })\n",
    "            \n",
    "            logits, states = model(**forward_kwargs)\n",
    "            # logits shape: [seq_len, 1, vocab_size]\n",
    "            \n",
    "            # Compute log probabilities\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Compute surprisal for each position\n",
    "            story_surprisals = [float('nan')] * len(words)  # Initialize with NaN\n",
    "            \n",
    "            for i in range(1, min(len(token_ids), len(words))):  # Skip first token (no context)\n",
    "                target_token_id = token_ids[i]\n",
    "                # Use prediction from step i-1 for token at step i\n",
    "                log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                surprisal = -log_prob / math.log(2)  # Convert to bits\n",
    "                story_surprisals[i] = surprisal\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            surprisal_value = story_surprisals[idx] if idx < len(story_surprisals) else float('nan')\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': surprisal_value\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_with_chunking_transformer(lightning_model, stories_df, tokenizer, chunk_size=64):\n",
    "    \"\"\"\n",
    "    Compute surprisal using the same chunking as training for SimpleTransformer model\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning SimpleTransformerLM\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (64)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    lightning_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Extract the actual SimpleTransformer model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Process in chunks matching training\n",
    "        story_surprisals = [float('nan')] * len(words)  # Initialize with NaN\n",
    "        \n",
    "        for start_idx in range(0, len(token_ids), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "            chunk_ids = token_ids[start_idx:end_idx]\n",
    "            \n",
    "            if len(chunk_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensor [seq_len, batch_size=1] (SimpleTransformer expects this format)\n",
    "            input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                # Prepare forward arguments\n",
    "                forward_kwargs = {\n",
    "                    'src': input_tensor\n",
    "                }\n",
    "                \n",
    "                # Add Gumbel parameters if model was trained with them\n",
    "                if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                    forward_kwargs.update({\n",
    "                        'temperature': 0.1,  # Use moderate temp for inference\n",
    "                        'use_gumbel': True,  # Don't use Gumbel sampling for surprisal\n",
    "                    })\n",
    "                \n",
    "                # Get logits: [seq_len, batch_size=1, vocab_size]\n",
    "                logits = model(**forward_kwargs)\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Compute surprisal for each position in chunk\n",
    "                for i in range(1, len(chunk_ids)):  # Skip first token (no previous context)\n",
    "                    target_token_id = chunk_ids[i]\n",
    "                    \n",
    "                    # Use prediction from step i-1 for token at step i\n",
    "                    # logits[i-1, 0, :] contains predictions for position i\n",
    "                    log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                    surprisal = -log_prob/math.log(2)  # Surprisal = -log(probability)\n",
    "                    \n",
    "                    # Map back to story position\n",
    "                    story_pos = start_idx + i\n",
    "                    if story_pos < len(story_surprisals):\n",
    "                        story_surprisals[story_pos] = surprisal\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer.load('tokenizer.json')\n",
    "checkpoints = {\n",
    "    'transformer_128_1_false':'/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_000/lightning_logs/version_1356201/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_1_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_001/lightning_logs/version_1356202/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_128_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_002/lightning_logs/version_1356203/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_003/lightning_logs/version_1356204/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_004/lightning_logs/version_1356205/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_005/lightning_logs/version_1356206/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_006/lightning_logs/version_1356207/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_8_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_007/lightning_logs/version_1356197/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_1_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_000/lightning_logs/version_1364335/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_1_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_001/lightning_logs/version_1364336/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_002/lightning_logs/version_1364337/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_003/lightning_logs/version_1364338/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_004/lightning_logs/version_1364339/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_005/lightning_logs/version_1364340/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_8_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_006/lightning_logs/version_1364341/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_8_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_007/lightning_logs/version_1364333/checkpoints/epoch=49-step=309500.ckpt'\n",
    "    # 'lstm_128' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_lstm_000/lightning_logs/version_1356004/checkpoints/epoch=49-step=309500.ckpt',\n",
    "}\n",
    "\n",
    "# Placeholder dictionary to store ΔLogLik\n",
    "delta_loglik_dict = {}\n",
    "\n",
    "# Load your reading time and surprisal data\n",
    "# (You need to have these prepared for your analysis)\n",
    "rt_data = pd.read_csv('processed_RTs.tsv', sep='\\t')\n",
    "\n",
    "# Loop over checkpoints\n",
    "for name, ckpt_path in checkpoints.items():\n",
    "    print(f\"\\nRunning analysis for checkpoint: {name}\")\n",
    "    if name.startswith(\"transformer\"):\n",
    "        model = load_trained_transformer(ckpt_path)\n",
    "        surprisal_data = compute_surprisal_with_chunking_transformer(model, stories, tokenizer=tokenizer)\n",
    "\n",
    "    elif name.startswith(\"cbr\"):\n",
    "        model = load_trained_cbr(ckpt_path)\n",
    "        surprisal_data = compute_surprisal_with_chunking_cbr(model, stories, tokenizer=tokenizer)\n",
    "\n",
    "    # Run ΔLogLik analysis\n",
    "    results = run_paper_analysis_correct(rt_data, surprisal_data)\n",
    "    \n",
    "    # Store ΔLogLik\n",
    "    delta_loglik_dict[name] = results['delta_loglik']\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== ΔLogLik for all checkpoints ===\")\n",
    "for name, delta in delta_loglik_dict.items():\n",
    "    print(f\"{name}: {delta:.2f}\")\n",
    "\n",
    "with open(\"delta_loglik_results.json\", \"w\") as f:\n",
    "    json.dump(delta_loglik_dict, f, indent=4)\n",
    "\n",
    "print(\"\\nΔLogLik results saved to delta_loglik_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== ΔLogLik Ranking (Best to Worst) ===\")\n",
    "sorted_results = sorted(delta_loglik_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for rank, (name, delta) in enumerate(sorted_results, 1):\n",
    "    print(f\"{rank:2d}. {name:<25} ΔLogLik: {delta:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_window_transformer(lightning_model, stories_df, tokenizer, chunk_size=64, step_size=16):\n",
    "    \"\"\"\n",
    "    Compute surprisal using overlapping chunks and averaging for SimpleTransformer model\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning SimpleTransformerLM\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (64)\n",
    "        step_size: Step size between overlapping windows\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    lightning_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Extract the actual SimpleTransformer model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Initialize surprisal accumulation\n",
    "        surprisal_sums = [0.0] * len(words)\n",
    "        surprisal_counts = [0] * len(words)\n",
    "        \n",
    "        # Generate multiple overlapping windows\n",
    "        for start_offset in range(0, min(chunk_size, len(token_ids)), step_size):\n",
    "            for start_idx in range(start_offset, len(token_ids) - 1, chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "                chunk_ids = token_ids[start_idx:end_idx]\n",
    "                \n",
    "                if len(chunk_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "                    continue\n",
    "                \n",
    "                # Convert to tensor [seq_len, batch_size=1]\n",
    "                input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.no_grad():\n",
    "                    forward_kwargs = {\n",
    "                        'src': input_tensor\n",
    "                    }\n",
    "                    \n",
    "                    # Add Gumbel parameters if model was trained with them\n",
    "                    if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                        forward_kwargs.update({\n",
    "                            'temperature': 0.1,\n",
    "                            'use_gumbel': False  # Don't use Gumbel for surprisal computation\n",
    "                        })\n",
    "                    \n",
    "                    # Get logits: [seq_len, batch_size=1, vocab_size]\n",
    "                    logits = model(**forward_kwargs)\n",
    "                    \n",
    "                    # Compute log probabilities\n",
    "                    log_probs = F.log_softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # Accumulate surprisals for this chunk\n",
    "                    for i in range(1, len(chunk_ids)):  # Skip first token\n",
    "                        target_token_id = chunk_ids[i]\n",
    "                        \n",
    "                        # Use prediction from step i-1 for token at step i\n",
    "                        log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                        surprisal = -log_prob / math.log(2)  # Convert to bits\n",
    "                        \n",
    "                        # Map back to story position\n",
    "                        story_pos = start_idx + i\n",
    "                        if story_pos < len(surprisal_sums):\n",
    "                            surprisal_sums[story_pos] += surprisal\n",
    "                            surprisal_counts[story_pos] += 1\n",
    "        \n",
    "        # Compute averaged surprisals\n",
    "        story_surprisals = []\n",
    "        for i in range(len(words)):\n",
    "            if surprisal_counts[i] > 0:\n",
    "                avg_surprisal = surprisal_sums[i] / surprisal_counts[i]\n",
    "                story_surprisals.append(avg_surprisal)\n",
    "            else:\n",
    "                story_surprisals.append(float('nan'))\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_window_cbr(lightning_model, stories_df, tokenizer, chunk_size=64, step_size=16, num_samples=4):\n",
    "    \"\"\"\n",
    "    Compute surprisal using overlapping chunks and averaging\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning CBRLanguageModel\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (64)\n",
    "        step_size: Step size between overlapping windows\n",
    "        num_samples: Number of different starting positions to sample per token\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    \n",
    "    # Extract the actual CBR_RNN model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Initialize surprisal accumulation\n",
    "        surprisal_sums = [0.0] * len(words)\n",
    "        surprisal_counts = [0] * len(words)\n",
    "        \n",
    "        # Generate multiple overlapping windows\n",
    "        for start_offset in range(0, min(chunk_size, len(token_ids)), step_size):\n",
    "            for start_idx in range(start_offset, len(token_ids) - 1, chunk_size):\n",
    "                end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "                chunk_ids = token_ids[start_idx:end_idx]\n",
    "                \n",
    "                if len(chunk_ids) < 2:  # Need at least 2 tokens\n",
    "                    continue\n",
    "                \n",
    "                # Convert to tensor [chunk_len, batch_size=1]\n",
    "                input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "                \n",
    "                # Initialize cache for this chunk\n",
    "                initial_cache = model.init_cache(input_tensor)\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.no_grad():\n",
    "                    forward_kwargs = {\n",
    "                        'observation': input_tensor,\n",
    "                        'initial_cache': initial_cache\n",
    "                    }\n",
    "                    \n",
    "                    if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                        forward_kwargs.update({\n",
    "                            'temperature': 0.1,\n",
    "                            'use_gumbel': False\n",
    "                        })\n",
    "                    \n",
    "                    logits, states = model(**forward_kwargs)\n",
    "                    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # Accumulate surprisals for this chunk\n",
    "                    for i in range(1, len(chunk_ids)):  # Skip first token\n",
    "                        target_token_id = chunk_ids[i]\n",
    "                        log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                        surprisal = -log_prob / math.log(2)\n",
    "                        \n",
    "                        # Map back to story position\n",
    "                        story_pos = start_idx + i\n",
    "                        if story_pos < len(surprisal_sums):\n",
    "                            surprisal_sums[story_pos] += surprisal\n",
    "                            surprisal_counts[story_pos] += 1\n",
    "        \n",
    "        # Compute averaged surprisals\n",
    "        story_surprisals = []\n",
    "        for i in range(len(words)):\n",
    "            if surprisal_counts[i] > 0:\n",
    "                avg_surprisal = surprisal_sums[i] / surprisal_counts[i]\n",
    "                story_surprisals.append(avg_surprisal)\n",
    "            else:\n",
    "                story_surprisals.append(float('nan'))\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer.load('tokenizer.json')\n",
    "checkpoints = {\n",
    "    'transformer_128_1_false':'/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_000/lightning_logs/version_1356201/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_1_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_001/lightning_logs/version_1356202/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_128_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_002/lightning_logs/version_1356203/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_003/lightning_logs/version_1356204/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_004/lightning_logs/version_1356205/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_005/lightning_logs/version_1356206/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_006/lightning_logs/version_1356207/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'transformer_512_8_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_transformer_007/lightning_logs/version_1356197/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_1_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_000/lightning_logs/version_1364335/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_1_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_001/lightning_logs/version_1364336/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_002/lightning_logs/version_1364337/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_8_false' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_003/lightning_logs/version_1364338/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_004/lightning_logs/version_1364339/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_1_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_005/lightning_logs/version_1364340/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_128_8_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_006/lightning_logs/version_1364341/checkpoints/epoch=49-step=309500.ckpt',\n",
    "    'cbr_512_8_true' : '/scratch2/mrenaudin/Hard-CBR-RNN/job_cbr_007/lightning_logs/version_1364333/checkpoints/epoch=49-step=309500.ckpt'\n",
    "    # 'lstm_128' : '/scratch2/mrenaudin/Hard-CBR-RNN/final_models/job_lstm_000/lightning_logs/version_1356004/checkpoints/epoch=49-step=309500.ckpt',\n",
    "}\n",
    "\n",
    "# Placeholder dictionary to store ΔLogLik\n",
    "delta_loglik_dict = {}\n",
    "\n",
    "# Load your reading time and surprisal data\n",
    "# (You need to have these prepared for your analysis)\n",
    "rt_data = pd.read_csv('processed_RTs.tsv', sep='\\t')\n",
    "\n",
    "# Loop over checkpoints\n",
    "for name, ckpt_path in checkpoints.items():\n",
    "    print(f\"\\nRunning analysis for checkpoint: {name}\")\n",
    "    if name.startswith(\"transformer\"):\n",
    "        model = load_trained_transformer(ckpt_path)\n",
    "        surprisal_data = compute_surprisal_window_transformer(model, stories, tokenizer=tokenizer)\n",
    "\n",
    "    elif name.startswith(\"cbr\"):\n",
    "        model = load_trained_cbr(ckpt_path)\n",
    "        surprisal_data = compute_surprisal_window_cbr(model, stories, tokenizer=tokenizer)\n",
    "\n",
    "    # Run ΔLogLik analysis\n",
    "    results = run_paper_analysis_correct(rt_data, surprisal_data)\n",
    "    \n",
    "    # Store ΔLogLik\n",
    "    delta_loglik_dict[name] = results['delta_loglik']\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== ΔLogLik for all checkpoints ===\")\n",
    "for name, delta in delta_loglik_dict.items():\n",
    "    print(f\"{name}: {delta:.2f}\")\n",
    "\n",
    "# with open(\"delta_loglik_results.json\", \"w\") as f:\n",
    "#     json.dump(delta_loglik_dict, f, indent=4)\n",
    "\n",
    "# print(\"\\nΔLogLik results saved to delta_loglik_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaps3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
