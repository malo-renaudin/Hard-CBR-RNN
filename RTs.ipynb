{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.read_csv('/scratch2/mrenaudin/Hard-CBR-RNN/all_stories.tok', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict surprisal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from original_cbr import CBR_RNN\n",
    "checkpoint = '/scratch2/mrenaudin/Hard-CBR-RNN/job_007/lightning_logs/version_1198526/checkpoints/epoch=49-step=565950.ckpt'\n",
    "hparams_path = '/scratch2/mrenaudin/Hard-CBR-RNN/job_007/lightning_logs/version_1198526/hparams.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "with open(hparams_path, 'r') as f:\n",
    "            hparams = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = hparams['vocab_size']+1  # vocabulary size\n",
    "ninp = hparams['ninp']      # embedding dimension\n",
    "nhid = hparams['nhid']      # hidden dimension\n",
    "nlayers = hparams.get('nlayers', 1)  # number of layers\n",
    "nheads = hparams.get('nheads', 1)    # number of attention heads\n",
    "dropout = hparams.get('dropout', 0.5) # dropout rate\n",
    "\n",
    "# Initialize model\n",
    "model = CBR_RNN(\n",
    "    ntoken=ntoken,\n",
    "    ninp=ninp, \n",
    "    nhid=nhid,\n",
    "    nlayers=nlayers,\n",
    "    nheads=nheads,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint, map_location='cpu')\n",
    "\n",
    "# Extract state_dict (might be nested in Lightning checkpoint)\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    # Remove Lightning module prefix if present\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from original_cbr_lightning import CBRLanguageModel\n",
    "from grid_search import WordTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '/scratch2/mrenaudin/Hard-CBR-RNN/job_004/lightning_logs/version_1198535/checkpoints/epoch=49-step=565950.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path):\n",
    "    \"\"\"Load the trained Lightning model\"\"\"\n",
    "    model = CBRLanguageModel.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprisal_with_chunking(lightning_model, stories_df, tokenizer, chunk_size=35):\n",
    "    \"\"\"\n",
    "    Compute surprisal using the same chunking as training (seq_len=35)\n",
    "    \n",
    "    Args:\n",
    "        lightning_model: Loaded Lightning CBRLanguageModel\n",
    "        stories_df: DataFrame with columns [word, zone, item]\n",
    "        tokenizer: WordTokenizer object\n",
    "        chunk_size: Sequence length used during training (35)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added 'surprisal' column\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lightning_model.to(device)\n",
    "    \n",
    "    # Extract the actual CBR_RNN model\n",
    "    model = lightning_model.model\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each story\n",
    "    for story_id in sorted(stories_df['item'].unique()):\n",
    "        story_data = stories_df[stories_df['item'] == story_id].sort_values('zone')\n",
    "        words = story_data['word'].tolist()\n",
    "        \n",
    "        print(f\"Processing story {story_id} ({len(words)} words)...\")\n",
    "        \n",
    "        # Convert words to token IDs using the tokenizer\n",
    "        token_ids = []\n",
    "        for word in words:\n",
    "            token_id = tokenizer.stoi.get(word, 0)  # 0 = <unk>\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Process in chunks of 35 tokens (matching training)\n",
    "        story_surprisals = [float('nan')] * len(words)  # Initialize with NaN\n",
    "        \n",
    "        for start_idx in range(0, len(token_ids), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(token_ids))\n",
    "            chunk_ids = token_ids[start_idx:end_idx]\n",
    "            \n",
    "            if len(chunk_ids) < 2:  # Need at least 2 tokens for prediction\n",
    "                continue\n",
    "            \n",
    "            # Convert to tensor [chunk_len, batch_size=1]\n",
    "            input_tensor = torch.tensor(chunk_ids).unsqueeze(1).to(device)\n",
    "            \n",
    "            # Initialize cache for this chunk\n",
    "            initial_cache = model.init_cache(input_tensor)\n",
    "            \n",
    "            # Forward pass (same as training)\n",
    "            with torch.no_grad():\n",
    "                # Use same parameters as training\n",
    "                forward_kwargs = {\n",
    "                    'observation': input_tensor,\n",
    "                    'initial_cache': initial_cache\n",
    "                }\n",
    "                \n",
    "                # Add Gumbel parameters if model was trained with them\n",
    "                if hasattr(lightning_model, 'use_gumbel_softmax') and lightning_model.use_gumbel_softmax:\n",
    "                    forward_kwargs.update({\n",
    "                        'temperature': 0.1,  # Use fixed temp for inference\n",
    "                        'use_gumbel': True   # Don't use Gumbel for inference\n",
    "                    })\n",
    "                \n",
    "                logits, states = model(**forward_kwargs)\n",
    "                # logits shape: [chunk_len, 1, vocab_size]\n",
    "                \n",
    "                # Compute log probabilities\n",
    "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Compute surprisal for each position in chunk\n",
    "                for i in range(1, len(chunk_ids)):  # Skip first token (no context)\n",
    "                    target_token_id = chunk_ids[i]\n",
    "                    # Use prediction from step i-1 for token at step i\n",
    "                    log_prob = log_probs[i-1, 0, target_token_id].item()\n",
    "                    surprisal = -log_prob\n",
    "                    \n",
    "                    # Map back to story position\n",
    "                    story_pos = start_idx + i\n",
    "                    if story_pos < len(story_surprisals):\n",
    "                        story_surprisals[story_pos] = surprisal\n",
    "        \n",
    "        # Collect results for this story\n",
    "        for idx, (_, row) in enumerate(story_data.iterrows()):\n",
    "            results.append({\n",
    "                'word': row['word'],\n",
    "                'zone': row['zone'],\n",
    "                'item': row['item'],\n",
    "                'surprisal': story_surprisals[idx]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer.load('tokenizer.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisal = compute_surprisal_with_chunking(model, stories, tokenizer, chunk_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for reading time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data = pd.read_csv('/scratch2/mrenaudin/Hard-CBR-RNN/processed_RTs.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rt_data.merge(surprisal, on=['item', 'zone'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log_RT'] = np.log(data['RT'])                    # y variable\n",
    "data['word_length'] = data['word_x'].str.len()           # create length from word\n",
    "\n",
    "# Clean grouping variable - convert to sequential integers\n",
    "data['subject'] = pd.Categorical(data['WorkerId']).codes\n",
    "\n",
    "# Standardize predictors  \n",
    "data['surprisal_z'] = (data['surprisal'] - data['surprisal'].mean()) / data['surprisal'].std()\n",
    "data['length_z'] = (data['word_length'] - data['word_length'].mean()) / data['word_length'].std()\n",
    "\n",
    "# Reset index to avoid indexing issues\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data prepared: {len(data)} observations\")\n",
    "print(f\"Number of subjects: {data['subject'].nunique()}\")\n",
    "print(f\"Mean word length: {data['word_length'].mean():.1f} characters\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "\n",
    "y = data['log_RT'].values\n",
    "X = data[['surprisal_z', 'length_z']].values\n",
    "X = np.column_stack([np.ones(len(X)), X])  # Add intercept\n",
    "groups = data['subject'].values\n",
    "\n",
    "# Remove any NaN values\n",
    "mask = ~(np.isnan(y) | np.isnan(X).any(axis=1) | np.isnan(groups))\n",
    "y = y[mask]\n",
    "X = X[mask]\n",
    "groups = groups[mask]\n",
    "\n",
    "print(f\"Clean data: {len(y)} observations\")\n",
    "\n",
    "model = MixedLM(y, X, groups=groups)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(fitted_model):\n",
    "    \"\"\"Print key results\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"READING TIME PREDICTION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check if it's a fitted model\n",
    "    if hasattr(fitted_model, 'summary'):\n",
    "        print(fitted_model.summary().tables[1])  # Coefficients table only\n",
    "        \n",
    "        # Get coefficients using array indexing\n",
    "        surprisal_coef = fitted_model.params[1]  # x1 = surprisal\n",
    "        surprisal_p = fitted_model.pvalues[1]\n",
    "        \n",
    "        length_coef = fitted_model.params[2]     # x2 = word length  \n",
    "        length_p = fitted_model.pvalues[2]\n",
    "            \n",
    "        print(f\"\\nSurprisal coefficient (x1): {surprisal_coef:.4f}\")\n",
    "        print(f\"P-value: {surprisal_p:.4f}\")\n",
    "        print(f\"Significant: {'Yes' if surprisal_p < 0.05 else 'No'}\")\n",
    "        \n",
    "        print(f\"\\nWord length coefficient (x2): {length_coef:.4f}\")\n",
    "        print(f\"P-value: {length_p:.4f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if surprisal_coef > 0:\n",
    "            print(\"‚úì Higher surprisal ‚Üí Longer reading times (expected)\")\n",
    "        else:\n",
    "            print(\"‚ö† Higher surprisal ‚Üí Shorter reading times (unexpected)\")\n",
    "            print(\"  This suggests an issue with surprisal computation or model\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Model not properly fitted!\")\n",
    "        print(f\"Model type: {type(fitted_model)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following methodology from clark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge RT and surprisal data\n",
    "data = rt_data.merge(surprisal, on=['item', 'zone'], how='inner')\n",
    "\n",
    "# Filter for correct responses and reasonable RTs\n",
    "data = data[(data['correct'] >= 5) & (data['RT'] >= 100) & (data['RT'] <= 3000)]\n",
    "\n",
    "# Sort by item and zone to ensure proper ordering for spillover\n",
    "data = data.sort_values(['item', 'zone']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Data after filtering: {len(data)} observations\")\n",
    "\n",
    "# Create baseline predictors exactly as in paper\n",
    "data['word_length'] = data['word_x'].str.len()  # Word length in characters\n",
    "data['word_position'] = data['zone']          # Index of word position within sentence\n",
    "\n",
    "# Create unigram surprisal (simplified - you'd normally use KenLM on OpenWebText)\n",
    "# For now, using word frequency as proxy for unigram surprisal\n",
    "word_counts = data['word_x'].value_counts()\n",
    "total_words = len(data)\n",
    "data['word_freq'] = data['word_x'].map(word_counts)\n",
    "data['unigram_surprisal'] = -np.log(data['word_freq'] / total_words)\n",
    "\n",
    "# Current word surprisal (your model's output)\n",
    "data['current_surprisal'] = data['surprisal']\n",
    "\n",
    "# Previous word surprisal (spillover effects)\n",
    "data['prev_surprisal'] = data.groupby('item')['surprisal'].shift(1)\n",
    "\n",
    "# Remove rows where previous word spillover is NaN (first word of each story)\n",
    "data = data.dropna(subset=['prev_surprisal']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Data after spillover calculation: {len(data)} observations\")\n",
    "\n",
    "# Create subject grouping variable\n",
    "data['subject'] = pd.Categorical(data['WorkerId']).codes\n",
    "\n",
    "# Use RAW reading times as specified in paper (not log-transformed)\n",
    "data['RT_raw'] = data['RT']\n",
    "\n",
    "print(f\"Final dataset: {len(data)} observations, {data['subject'].nunique()} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_baseline_model(data):\n",
    "    \"\"\"\n",
    "    Fit baseline model without surprisal predictors\n",
    "    Use manual array approach for consistency\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Attempting baseline model with manual arrays...\")\n",
    "    \n",
    "    try:\n",
    "        # Create design matrix: intercept, word_length, word_position, unigram_surprisal\n",
    "        X_baseline = np.column_stack([\n",
    "            np.ones(len(data)),                    # intercept\n",
    "            data['word_length'].values,            # word length\n",
    "            data['word_position'].values,          # word position\n",
    "            data['unigram_surprisal'].values       # unigram surprisal\n",
    "        ])\n",
    "        \n",
    "        y = data['RT_raw'].values\n",
    "        groups = data['subject'].values\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        mask = ~(np.isnan(y) | np.isnan(X_baseline).any(axis=1) | np.isnan(groups))\n",
    "        y = y[mask]\n",
    "        X_baseline = X_baseline[mask]\n",
    "        groups = groups[mask]\n",
    "        \n",
    "        print(f\"Clean data for baseline: {len(y)} observations\")\n",
    "        print(f\"Baseline design matrix shape: {X_baseline.shape}\")\n",
    "        \n",
    "        # Fit with random intercepts\n",
    "        model = MixedLM(y, X_baseline, groups=groups)\n",
    "        result = model.fit(method='lbfgs')\n",
    "        print(\"Baseline model fitted successfully\")\n",
    "        \n",
    "        # Add coefficient names\n",
    "        result.params_names = ['intercept', 'word_length', 'word_position', 'unigram_surprisal']\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Baseline model failed: {e}\")\n",
    "        \n",
    "        # Fallback to OLS\n",
    "        print(\"Using OLS for baseline model...\")\n",
    "        import statsmodels.api as sm\n",
    "        ols_result = sm.OLS(y, X_baseline).fit()\n",
    "        ols_result.params_names = ['intercept', 'word_length', 'word_position', 'unigram_surprisal']\n",
    "        \n",
    "        return ols_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_full_model(data):\n",
    "    \"\"\"\n",
    "    Fit full model with current and previous word surprisal\n",
    "    Use manual array approach to avoid formula parsing issues\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Attempting full model with manual arrays...\")\n",
    "    \n",
    "    try:\n",
    "        # Create design matrix manually\n",
    "        # Fixed effects: intercept, word_length, word_position, unigram_surprisal, current_surprisal, prev_surprisal\n",
    "        X = np.column_stack([\n",
    "            np.ones(len(data)),                    # intercept\n",
    "            data['word_length'].values,            # word length\n",
    "            data['word_position'].values,          # word position  \n",
    "            data['unigram_surprisal'].values,      # unigram surprisal\n",
    "            data['current_surprisal'].values,      # current surprisal\n",
    "            data['prev_surprisal'].values          # previous surprisal\n",
    "        ])\n",
    "        \n",
    "        y = data['RT_raw'].values\n",
    "        groups = data['subject'].values\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        mask = ~(np.isnan(y) | np.isnan(X).any(axis=1) | np.isnan(groups))\n",
    "        y = y[mask]\n",
    "        X = X[mask]\n",
    "        groups = groups[mask]\n",
    "        \n",
    "        print(f\"Clean data for full model: {len(y)} observations\")\n",
    "        print(f\"Design matrix shape: {X.shape}\")\n",
    "        \n",
    "        # Start with simplest random effects and build up\n",
    "        try:\n",
    "            # Try with random intercepts only first\n",
    "            model = MixedLM(y, X, groups=groups)\n",
    "            result = model.fit(method='lbfgs')\n",
    "            print(\"Full model fitted with random intercepts only\")\n",
    "            \n",
    "            # Add coefficient names for interpretation\n",
    "            result.params_names = ['intercept', 'word_length', 'word_position', \n",
    "                                 'unigram_surprisal', 'current_surprisal', 'prev_surprisal']\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Even simple random effects failed: {e}\")\n",
    "            \n",
    "            # Ultimate fallback: OLS\n",
    "            print(\"Falling back to OLS for full model...\")\n",
    "            import statsmodels.api as sm\n",
    "            ols_result = sm.OLS(y, X).fit()\n",
    "            \n",
    "            # Add names for interpretation\n",
    "            ols_result.params_names = ['intercept', 'word_length', 'word_position',\n",
    "                                     'unigram_surprisal', 'current_surprisal', 'prev_surprisal']\n",
    "            \n",
    "            return ols_result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Manual array approach failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta_loglik(baseline_result, full_result):\n",
    "    \"\"\"\n",
    "    Calculate ‚àÜLogLik as in the paper - robust version\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get log-likelihood values\n",
    "    if hasattr(baseline_result, 'llf'):\n",
    "        baseline_ll = baseline_result.llf\n",
    "    else:\n",
    "        baseline_ll = getattr(baseline_result, 'loglike', np.nan)\n",
    "        \n",
    "    if hasattr(full_result, 'llf'):\n",
    "        full_ll = full_result.llf  \n",
    "    else:\n",
    "        full_ll = getattr(full_result, 'loglike', np.nan)\n",
    "    \n",
    "    delta_loglik = full_ll - baseline_ll\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON RESULTS (Following Paper)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Baseline Log-Likelihood: {baseline_ll:.2f}\")\n",
    "    print(f\"Full Model Log-Likelihood: {full_ll:.2f}\")\n",
    "    print(f\"‚àÜLogLik (improvement): {delta_loglik:.2f}\")\n",
    "    \n",
    "    if not np.isnan(delta_loglik):\n",
    "        # Likelihood ratio test\n",
    "        lr_stat = 2 * delta_loglik\n",
    "        df_diff = len(full_result.params) - len(baseline_result.params)\n",
    "        \n",
    "        from scipy.stats import chi2\n",
    "        p_value = 1 - chi2.cdf(lr_stat, df_diff)\n",
    "        \n",
    "        print(f\"Likelihood Ratio Test: œá¬≤ = {lr_stat:.2f}, df = {df_diff}, p = {p_value:.4f}\")\n",
    "        print(f\"Significance: {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'n.s.'}\")\n",
    "        \n",
    "        return {\n",
    "            'delta_loglik': delta_loglik,\n",
    "            'lr_stat': lr_stat,\n",
    "            'p_value': p_value,\n",
    "            'baseline_ll': baseline_ll,\n",
    "            'full_ll': full_ll\n",
    "        }\n",
    "    else:\n",
    "        print(\"Could not compute likelihood ratio test\")\n",
    "        return {\n",
    "            'delta_loglik': delta_loglik,\n",
    "            'baseline_ll': baseline_ll,\n",
    "            'full_ll': full_ll\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full_model_results(full_result):\n",
    "    \"\"\"Print detailed results for the full model\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FULL MODEL COEFFICIENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if hasattr(full_result, 'summary'):\n",
    "        try:\n",
    "            print(full_result.summary().tables[1])\n",
    "        except:\n",
    "            # Manual printing if summary fails\n",
    "            print(\"Coefficients:\")\n",
    "            if hasattr(full_result, 'params_names'):\n",
    "                for i, name in enumerate(full_result.params_names):\n",
    "                    coef = full_result.params[i]\n",
    "                    pval = full_result.pvalues[i] if hasattr(full_result, 'pvalues') else 'N/A'\n",
    "                    print(f\"  {name}: {coef:.4f} (p = {pval})\")\n",
    "    \n",
    "    # Extract surprisal coefficients using our added names\n",
    "    if hasattr(full_result, 'params_names'):\n",
    "        names = full_result.params_names\n",
    "        params = full_result.params\n",
    "        pvalues = full_result.pvalues if hasattr(full_result, 'pvalues') else [np.nan] * len(params)\n",
    "        \n",
    "        # Find current and previous surprisal coefficients\n",
    "        current_idx = names.index('current_surprisal') if 'current_surprisal' in names else None\n",
    "        prev_idx = names.index('prev_surprisal') if 'prev_surprisal' in names else None\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*40)\n",
    "        print(\"SURPRISAL EFFECTS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        if current_idx is not None:\n",
    "            current_coef = params[current_idx]\n",
    "            current_p = pvalues[current_idx]\n",
    "            print(f\"Current word surprisal: {current_coef:.4f} (p = {current_p:.4f})\")\n",
    "            sig_current = '***' if current_p < 0.001 else '**' if current_p < 0.01 else '*' if current_p < 0.05 else 'n.s.'\n",
    "            print(f\"  Significance: {sig_current}\")\n",
    "            \n",
    "        if prev_idx is not None:\n",
    "            prev_coef = params[prev_idx]\n",
    "            prev_p = pvalues[prev_idx]\n",
    "            print(f\"Previous word surprisal (spillover): {prev_coef:.4f} (p = {prev_p:.4f})\")\n",
    "            sig_prev = '***' if prev_p < 0.001 else '**' if prev_p < 0.01 else '*' if prev_p < 0.05 else 'n.s.'\n",
    "            print(f\"  Significance: {sig_prev}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nCannot extract specific surprisal coefficients - check model structure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_paper_analysis(data):\n",
    "    \"\"\"\n",
    "    Complete analysis following the paper's methodology\n",
    "    \n",
    "    Returns model comparison results and fitted models\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING ANALYSIS FOLLOWING PAPER SPECIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Prepare data with all required predictors\n",
    "    # print(\"\\nStep 1: Preparing data...\")\n",
    "    # data = prepare_data_paper_spec(rt_data, surprisal_data)\n",
    "    \n",
    "    # Step 2: Fit baseline model (without surprisal)\n",
    "    print(\"\\nStep 2: Fitting baseline model...\")\n",
    "    baseline_result = fit_baseline_model(data)\n",
    "    \n",
    "    # Step 3: Fit full model (with current and previous surprisal)\n",
    "    print(\"\\nStep 3: Fitting full model...\")\n",
    "    full_result = fit_full_model(data)\n",
    "    \n",
    "    # Step 4: Calculate ‚àÜLogLik\n",
    "    print(\"\\nStep 4: Calculating model comparison...\")\n",
    "    comparison = calculate_delta_loglik(baseline_result, full_result)\n",
    "    \n",
    "    # Step 5: Print detailed results\n",
    "    print_full_model_results(full_result)\n",
    "    \n",
    "    return {\n",
    "        'data': data,\n",
    "        'baseline_model': baseline_result,\n",
    "        'full_model': full_result,\n",
    "        'comparison': comparison\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_paper_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final good methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_data_correct(rt_data, surprisal_data):\n",
    "    \"\"\"\n",
    "    Prepare data exactly as specified in paper\n",
    "    \"\"\"\n",
    "    # Merge and filter - handle duplicate column names\n",
    "    data = rt_data.merge(surprisal_data, on=['item', 'zone'], how='inner', suffixes=('_rt', '_surprisal'))\n",
    "    data = data[(data['correct'] >= 5) & (data['RT'] >= 100) & (data['RT'] <= 3000)]\n",
    "    data = data.sort_values(['item', 'zone']).reset_index(drop=True)\n",
    "    \n",
    "    # Use the word column from surprisal data (should be the same)\n",
    "    if 'word_surprisal' in data.columns:\n",
    "        data['word'] = data['word_surprisal']\n",
    "    elif 'word_rt' in data.columns:\n",
    "        data['word'] = data['word_rt']\n",
    "    elif 'word' in data.columns:\n",
    "        pass  # Already have word column\n",
    "    else:\n",
    "        # Check what columns we actually have\n",
    "        print(\"Available columns:\", data.columns.tolist())\n",
    "        raise ValueError(\"Cannot find word column after merge\")\n",
    "    \n",
    "    # Create exact predictors from paper\n",
    "    data['word_length'] = data['word'].str.len()\n",
    "    data['word_position'] = data['zone'] \n",
    "    \n",
    "    # Unigram surprisal - fix the duplicate index issue\n",
    "    word_counts = data['word'].value_counts()\n",
    "    total_words = len(data)\n",
    "    data['unigram_surprisal'] = data['word'].map(lambda w: -np.log(word_counts.get(w, 1) / total_words))\n",
    "    \n",
    "    # Current and previous word surprisal\n",
    "    data['current_surprisal'] = data['surprisal']\n",
    "    data['prev_surprisal'] = data.groupby('item')['surprisal'].shift(1)\n",
    "    \n",
    "    # Remove first words (no previous surprisal)\n",
    "    data = data.dropna(subset=['prev_surprisal']).reset_index(drop=True)\n",
    "    \n",
    "    # Subject coding\n",
    "    data['subject'] = pd.Categorical(data['WorkerId']).codes\n",
    "    \n",
    "    print(f\"Final data: {len(data)} observations, {data['subject'].nunique()} subjects\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def fit_simple_lmer(y, X, groups, max_attempts=3):\n",
    "    \"\"\"\n",
    "    Robust LME fitting with fallbacks to handle numerical issues\n",
    "    \"\"\"\n",
    "    from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # Clean data\n",
    "    mask = ~(np.isnan(y) | np.isnan(X).any(axis=1) | np.isnan(groups))\n",
    "    y_clean = y[mask]\n",
    "    X_clean = X[mask]\n",
    "    groups_clean = groups[mask]\n",
    "    \n",
    "    attempts = [\n",
    "        (\"MixedLM with random intercepts\", lambda: MixedLM(y_clean, X_clean, groups=groups_clean).fit(method='nm', maxiter=100)),\n",
    "        (\"MixedLM with BFGS\", lambda: MixedLM(y_clean, X_clean, groups=groups_clean).fit(method='bfgs', maxiter=50)),\n",
    "        (\"OLS fallback\", lambda: sm.OLS(y_clean, X_clean).fit())\n",
    "    ]\n",
    "    \n",
    "    for name, fit_func in attempts[:max_attempts]:\n",
    "        try:\n",
    "            result = fit_func()\n",
    "            if hasattr(result, 'llf') and np.isfinite(result.llf):\n",
    "                print(f\"  Fitted with {name}, LogLik = {result.llf:.2f}\")\n",
    "                return result\n",
    "            elif hasattr(result, 'llf'):\n",
    "                print(f\"  {name} converged but LogLik infinite\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {name} failed: {str(e)[:50]}...\")\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\"All fitting methods failed\")\n",
    "\n",
    "def compute_delta_loglik_correct(data):\n",
    "    \"\"\"\n",
    "    Compute ŒîLogLik exactly as in paper:\n",
    "    Compare baseline vs baseline + current_surprisal + prev_surprisal\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FITTING MODELS FOR ŒîLOGLIK CALCULATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare design matrices\n",
    "    y = data['RT'].values  # Raw RT as specified\n",
    "    \n",
    "    # Baseline: intercept + word_length + word_position + unigram_surprisal\n",
    "    X_baseline = np.column_stack([\n",
    "        np.ones(len(data)),\n",
    "        data['word_length'].values,\n",
    "        data['word_position'].values, \n",
    "        data['unigram_surprisal'].values\n",
    "    ])\n",
    "    \n",
    "    # Full: baseline + current_surprisal + prev_surprisal\n",
    "    X_full = np.column_stack([\n",
    "        X_baseline,\n",
    "        data['current_surprisal'].values,\n",
    "        data['prev_surprisal'].values\n",
    "    ])\n",
    "    \n",
    "    groups = data['subject'].values\n",
    "    \n",
    "    print(f\"Data shape: {len(y)} observations\")\n",
    "    print(f\"Baseline predictors: {X_baseline.shape[1]} (intercept + word_length + word_position + unigram_surprisal)\")\n",
    "    print(f\"Full predictors: {X_full.shape[1]} (baseline + current_surprisal + prev_surprisal)\")\n",
    "    \n",
    "    # Fit baseline model\n",
    "    print(\"\\nFitting baseline model...\")\n",
    "    baseline_result = fit_simple_lmer(y, X_baseline, groups)\n",
    "    \n",
    "    # Fit full model  \n",
    "    print(\"\\nFitting full model...\")\n",
    "    full_result = fit_simple_lmer(y, X_full, groups)\n",
    "    \n",
    "    # Calculate ŒîLogLik\n",
    "    baseline_ll = baseline_result.llf\n",
    "    full_ll = full_result.llf\n",
    "    delta_loglik = full_ll - baseline_ll\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"ŒîLOGLIK RESULTS (PAPER SPECIFICATION)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Baseline Log-Likelihood: {baseline_ll:.2f}\")\n",
    "    print(f\"Full Model Log-Likelihood: {full_ll:.2f}\")  \n",
    "    print(f\"ŒîLogLik: {delta_loglik:.2f}\")\n",
    "    \n",
    "    if np.isfinite(delta_loglik):\n",
    "        # Likelihood ratio test\n",
    "        lr_stat = 2 * delta_loglik\n",
    "        df_diff = X_full.shape[1] - X_baseline.shape[1]  # Should be 2 (current + prev surprisal)\n",
    "        p_value = 1 - stats.chi2.cdf(lr_stat, df_diff)\n",
    "        \n",
    "        print(f\"Likelihood Ratio Test: œá¬≤ = {lr_stat:.2f}, df = {df_diff}, p = {p_value:.6f}\")\n",
    "        significance = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'n.s.'\n",
    "        print(f\"Significance: {significance}\")\n",
    "        \n",
    "        # Effect interpretation\n",
    "        if delta_loglik > 0:\n",
    "            print(f\"‚úì Your CBR-RNN surprisal improves model fit by {delta_loglik:.2f} log-likelihood units\")\n",
    "        else:\n",
    "            print(f\"‚úó Surprisal does not improve model fit (ŒîLogLik = {delta_loglik:.2f})\")\n",
    "    else:\n",
    "        print(\"‚ö† ŒîLogLik calculation failed due to numerical issues\")\n",
    "        print(\"Models fitted but log-likelihood values are problematic\")\n",
    "    \n",
    "    # Extract surprisal coefficients from full model\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"SURPRISAL COEFFICIENTS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if hasattr(full_result, 'params') and len(full_result.params) >= 6:\n",
    "        current_coef = full_result.params[4]  # 5th coefficient (current surprisal)\n",
    "        prev_coef = full_result.params[5]     # 6th coefficient (previous surprisal)\n",
    "        \n",
    "        if hasattr(full_result, 'pvalues'):\n",
    "            current_p = full_result.pvalues[4]\n",
    "            prev_p = full_result.pvalues[5]\n",
    "        else:\n",
    "            current_p = prev_p = np.nan\n",
    "            \n",
    "        print(f\"Current word surprisal: {current_coef:.4f} (p = {current_p:.4f})\")\n",
    "        print(f\"Previous word surprisal: {prev_coef:.4f} (p = {prev_p:.4f})\")\n",
    "        \n",
    "        if current_coef > 0:\n",
    "            print(\"‚úì Current surprisal has expected positive effect\")\n",
    "        else:\n",
    "            print(\"‚ö† Current surprisal has unexpected negative effect\")\n",
    "    \n",
    "    return {\n",
    "        'delta_loglik': delta_loglik,\n",
    "        'baseline_model': baseline_result,\n",
    "        'full_model': full_result,\n",
    "        'lr_test': {\n",
    "            'statistic': lr_stat if 'lr_stat' in locals() else np.nan,\n",
    "            'p_value': p_value if 'p_value' in locals() else np.nan,\n",
    "            'df': df_diff if 'df_diff' in locals() else 2\n",
    "        }\n",
    "    }\n",
    "\n",
    "def run_paper_analysis_correct(rt_data, surprisal_data):\n",
    "    \"\"\"\n",
    "    Run the exact analysis from the paper to get ŒîLogLik\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)  \n",
    "    print(\"PAPER-COMPLIANT ŒîLOGLIK ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    data = prepare_data_correct(rt_data, surprisal_data)\n",
    "    \n",
    "    # Compute ŒîLogLik\n",
    "    results = compute_delta_loglik_correct(data)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ŒîLogLik for your CBR-RNN: {results['delta_loglik']:.2f}\")\n",
    "    \n",
    "    if np.isfinite(results['delta_loglik']):\n",
    "        if results['delta_loglik'] > 0:\n",
    "            print(\"üéâ Your model improves prediction of human reading times!\")\n",
    "            print(f\"Improvement: {results['delta_loglik']:.2f} log-likelihood units\")\n",
    "        else:\n",
    "            print(\"‚ùå Your model does not improve reading time prediction\")\n",
    "    else:\n",
    "        print(\"‚ö† Numerical issues prevented ŒîLogLik calculation\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data\n",
    "rt_data = pd.read_csv('processed_RTs.tsv', sep='\\t')\n",
    "surprisal_data = compute_surprisal_with_chunking(model, stories, tokenizer=tokenizer)\n",
    "\n",
    "# Get the ŒîLogLik metric from the paper\n",
    "results = run_paper_analysis_correct(rt_data, surprisal_data)\n",
    "delta_loglik = results['delta_loglik']\n",
    "\n",
    "print(f\"Your CBR-RNN ŒîLogLik: {delta_loglik:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_RTs import prepare_data_correct, fit_simple_lmer, compute_delta_loglik_correct, run_paper_analysis_correct\n",
    "\n",
    "check_128_1_false = '/scratch2/mrenaudin/Hard-CBR-RNN/job_000/lightning_logs/version_1198531/checkpoints/epoch=49-step=565950.ckpt'\n",
    "check_512_1_false = '/scratch2/mrenaudin/Hard-CBR-RNN/job_001/lightning_logs/version_1198532/checkpoints/epoch=49-step=565950.ckpt'\n",
    "check_128_8_false = '/scratch2/mrenaudin/Hard-CBR-RNN/job_002/lightning_logs/version_1198533/checkpoints/epoch=49-step=565950.ckpt'\n",
    "check_512_8_false = '/scratch2/mrenaudin/Hard-CBR-RNN/job_003/lightning_logs/version_1198534/checkpoints/epoch=49-step=565950.ckpt'\n",
    "check_128_1_true = '/scratch2/mrenaudin/Hard-CBR-RNN/job_004/lightning_logs/version_1198535/checkpoints/epoch=49-step=565950.ckpt'\n",
    "check_512_1_true = '/scratch2/mrenaudin/Hard-CBR-RNN/job_005/lightning_logs/version_1198536/checkpoints/epoch=49-step=565950.ckpt'\n",
    "check_128_8_true = '/scratch2/mrenaudin/Hard-CBR-RNN/job_006/lightning_logs/version_1198537/checkpoints/epoch=49-step=565950.ckpt'\n",
    "check_512_8_true = '/scratch2/mrenaudin/Hard-CBR-RNN/job_007/lightning_logs/version_1198526/checkpoints/epoch=49-step=565950.ckpt'\n",
    "\n",
    "tokenizer = WordTokenizer.load('tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_RTs import prepare_data_correct, fit_simple_lmer, compute_delta_loglik_correct, run_paper_analysis_correct\n",
    "from grid_search import WordTokenizer  # assuming your tokenizer class is here\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = WordTokenizer.load('tokenizer.json')\n",
    "\n",
    "# Define your checkpoints\n",
    "checkpoints = {\n",
    "    \"128_1_false\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_000/lightning_logs/version_1198531/checkpoints/epoch=49-step=565950.ckpt',\n",
    "    \"512_1_false\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_001/lightning_logs/version_1198532/checkpoints/epoch=49-step=565950.ckpt',\n",
    "    \"128_8_false\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_002/lightning_logs/version_1198533/checkpoints/epoch=49-step=565950.ckpt',\n",
    "    \"512_8_false\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_003/lightning_logs/version_1198534/checkpoints/epoch=49-step=565950.ckpt',\n",
    "    \"128_1_true\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_004/lightning_logs/version_1198535/checkpoints/epoch=49-step=565950.ckpt',\n",
    "    \"512_1_true\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_005/lightning_logs/version_1198536/checkpoints/epoch=49-step=565950.ckpt',\n",
    "    \"128_8_true\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_006/lightning_logs/version_1198537/checkpoints/epoch=49-step=565950.ckpt',\n",
    "    \"512_8_true\": '/scratch2/mrenaudin/Hard-CBR-RNN/job_007/lightning_logs/version_1198526/checkpoints/epoch=49-step=565950.ckpt'\n",
    "}\n",
    "\n",
    "# Placeholder dictionary to store ŒîLogLik\n",
    "delta_loglik_dict = {}\n",
    "\n",
    "# Load your reading time and surprisal data\n",
    "# (You need to have these prepared for your analysis)\n",
    "rt_data = pd.read_csv('processed_RTs.tsv', sep='\\t')\n",
    "\n",
    "# Loop over checkpoints\n",
    "for name, ckpt_path in checkpoints.items():\n",
    "    print(f\"\\nRunning analysis for checkpoint: {name}\")\n",
    "    model = load_trained_model(ckpt_path)\n",
    "    \n",
    "    surprisal_data = compute_surprisal_with_chunking(model, stories, tokenizer=tokenizer)\n",
    "    \n",
    "    # Run ŒîLogLik analysis\n",
    "    results = run_paper_analysis_correct(rt_data, surprisal_data)\n",
    "    \n",
    "    # Store ŒîLogLik\n",
    "    delta_loglik_dict[name] = results['delta_loglik']\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== ŒîLogLik for all checkpoints ===\")\n",
    "for name, delta in delta_loglik_dict.items():\n",
    "    print(f\"{name}: {delta:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_factorial_delta_loglik(delta_loglik_dict):\n",
    "    \"\"\"\n",
    "    Generate all plots for a 2x2x2 factorial design\n",
    "    :param delta_loglik_dict: dict with keys like '128_1_true' and ŒîLogLik values\n",
    "    \"\"\"\n",
    "    # --- Prepare DataFrame ---\n",
    "    rows = []\n",
    "    for name, delta in delta_loglik_dict.items():\n",
    "        hidden_dim, heads, gumbel = name.split(\"_\")\n",
    "        rows.append({\n",
    "            'hidden_dim': int(hidden_dim),\n",
    "            'heads': int(heads),\n",
    "            'gumbel_softmax': gumbel == 'true',\n",
    "            'delta_loglik': delta\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # --- 1. Main Effects ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    sns.barplot(data=df, x='hidden_dim', y='delta_loglik', ax=axes[0])\n",
    "    axes[0].set_title(\"Main Effect: Hidden Dimension\")\n",
    "    \n",
    "    sns.barplot(data=df, x='heads', y='delta_loglik', ax=axes[1])\n",
    "    axes[1].set_title(\"Main Effect: Number of Heads\")\n",
    "    \n",
    "    sns.barplot(data=df, x='gumbel_softmax', y='delta_loglik', ax=axes[2])\n",
    "    axes[2].set_title(\"Main Effect: Gumbel Softmax\")\n",
    "    axes[2].set_xticklabels([\"Classic\", \"Gumbel\"])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- 2. Two-way Interactions ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    sns.pointplot(data=df, x='hidden_dim', y='delta_loglik', hue='heads', ax=axes[0],\n",
    "                  dodge=True, markers=[\"o\", \"s\"], linestyles=[\"-\", \"--\"])\n",
    "    axes[0].set_title(\"Interaction: Hidden Dimension √ó Heads\")\n",
    "    \n",
    "    sns.pointplot(data=df, x='hidden_dim', y='delta_loglik', hue='gumbel_softmax', ax=axes[1],\n",
    "                  dodge=True, markers=[\"o\", \"s\"], linestyles=[\"-\", \"--\"])\n",
    "    axes[1].set_title(\"Interaction: Hidden Dimension √ó Softmax\")\n",
    "    axes[1].set_xticklabels([\"128\", \"512\"])\n",
    "    axes[1].legend(title=\"Gumbel Softmax\", labels=[\"Classic\", \"Gumbel\"])\n",
    "    \n",
    "    sns.pointplot(data=df, x='heads', y='delta_loglik', hue='gumbel_softmax', ax=axes[2],\n",
    "                  dodge=True, markers=[\"o\", \"s\"], linestyles=[\"-\", \"--\"])\n",
    "    axes[2].set_title(\"Interaction: Heads √ó Softmax\")\n",
    "    axes[2].legend(title=\"Gumbel Softmax\", labels=[\"Classic\", \"Gumbel\"])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- 3. Three-way Interaction ---\n",
    "    g = sns.catplot(\n",
    "        data=df, kind=\"bar\",\n",
    "        x=\"hidden_dim\", y=\"delta_loglik\", hue=\"heads\",\n",
    "        col=\"gumbel_softmax\", palette=\"muted\", ci=None,\n",
    "        height=5, aspect=0.8\n",
    "    )\n",
    "    g.set_axis_labels(\"Hidden Dimension\", \"ŒîLogLik\")\n",
    "    g.set_titles(\"Gumbel Softmax = {col_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # --- 4. Optional Heatmaps ---\n",
    "    for softmax_type, group_df in df.groupby('gumbel_softmax'):\n",
    "        heatmap_df = group_df.pivot(index='hidden_dim', columns='heads', values='delta_loglik')\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "        plt.title(f\"ŒîLogLik Heatmap - {'Gumbel' if softmax_type else 'Classic'} Softmax\")\n",
    "        plt.xlabel(\"Number of Heads\")\n",
    "        plt.ylabel(\"Hidden Dimension\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factorial_delta_loglik(delta_loglik_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {'model': name, 'delta_loglik': delta}\n",
    "    for name, delta in delta_loglik_dict.items()\n",
    "])\n",
    "\n",
    "# Sort by ŒîLogLik\n",
    "df = df.sort_values('delta_loglik').reset_index(drop=True)\n",
    "df['rank'] = df.index + 1  # for x-axis\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df['rank'], df['delta_loglik'], color='dodgerblue', s=100)\n",
    "plt.xticks(df['rank'], df['model'], rotation=45, ha='right')\n",
    "plt.xlabel(\"Model (sorted by ŒîLogLik)\")\n",
    "plt.ylabel(\"ŒîLogLik\")\n",
    "plt.title(\"Model improvements in Reading Time Prediction\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {'model': name, 'delta_loglik': delta}\n",
    "    for name, delta in delta_loglik_dict.items()\n",
    "])\n",
    "\n",
    "# Extract factors from the model name\n",
    "df[['hidden_dim', 'heads', 'gumbel']] = df['model'].str.split('_', expand=True)\n",
    "df['hidden_dim'] = df['hidden_dim'].astype(int)\n",
    "df['heads'] = df['heads'].astype(int)\n",
    "df['gumbel'] = df['gumbel'] == 'true'\n",
    "# Hidden dimension effect\n",
    "hidden_effect = df.groupby('hidden_dim')['delta_loglik'].mean().sort_values(ascending=False)\n",
    "# Heads effect\n",
    "heads_effect = df.groupby('heads')['delta_loglik'].mean().sort_values(ascending=False)\n",
    "# Gumbel effect\n",
    "gumbel_effect = df.groupby('gumbel')['delta_loglik'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Hidden dimension effect (higher ŒîLogLik = better):\")\n",
    "print(hidden_effect)\n",
    "print(\"\\nNumber of heads effect:\")\n",
    "print(heads_effect)\n",
    "print(\"\\nGumbel softmax effect:\")\n",
    "print(gumbel_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "factors = ['hidden_dim', 'heads', 'gumbel']\n",
    "effects = [hidden_effect, heads_effect, gumbel_effect]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for i, effect in enumerate(effects):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    effect.plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'{factors[i]} effect')\n",
    "    plt.ylabel('Mean ŒîLogLik')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {'model': name, 'delta_loglik': delta}\n",
    "    for name, delta in delta_loglik_dict.items()\n",
    "])\n",
    "\n",
    "# Extract factors from the model name\n",
    "df[['hidden_dim', 'heads', 'gumbel']] = df['model'].str.split('_', expand=True)\n",
    "df['hidden_dim'] = df['hidden_dim'].astype(int)\n",
    "df['heads'] = df['heads'].astype(int)\n",
    "df['gumbel'] = df['gumbel'] == 'true'\n",
    "\n",
    "# --- Step 1: Compute mean ŒîLogLik for each factor level ---\n",
    "hidden_effect = df.groupby('hidden_dim')['delta_loglik'].mean().reset_index()\n",
    "hidden_effect['factor'] = 'hidden_dim'\n",
    "hidden_effect.rename(columns={'hidden_dim': 'level', 'delta_loglik': 'mean_delta'}, inplace=True)\n",
    "\n",
    "heads_effect = df.groupby('heads')['delta_loglik'].mean().reset_index()\n",
    "heads_effect['factor'] = 'heads'\n",
    "heads_effect.rename(columns={'heads': 'level', 'delta_loglik': 'mean_delta'}, inplace=True)\n",
    "\n",
    "gumbel_effect = df.groupby('gumbel')['delta_loglik'].mean().reset_index()\n",
    "gumbel_effect['factor'] = 'gumbel_softmax'\n",
    "gumbel_effect.rename(columns={'gumbel': 'level', 'delta_loglik': 'mean_delta'}, inplace=True)\n",
    "\n",
    "# Combine all factors into a single DataFrame\n",
    "effects_df = pd.concat([hidden_effect, heads_effect, gumbel_effect], ignore_index=True)\n",
    "\n",
    "# --- Step 2: Rank all factor levels by mean ŒîLogLik ---\n",
    "effects_df = effects_df.sort_values('mean_delta', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Hierarchy of constraints by contribution to ŒîLogLik:\")\n",
    "print(effects_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaps3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
