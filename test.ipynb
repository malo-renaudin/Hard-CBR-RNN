{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc76169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrenaudin/.conda/envs/leaps3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data.utils.data_utils import WordTokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import math\n",
    "from models import CBR_LM, Transformer_LM, LSTM_LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a6b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NounPPDataset(Dataset):\n",
    "    def __init__(self, nounpp_file, tokenizer):\n",
    "        self.sentences = []\n",
    "        self.conditions = []\n",
    "        self.correct = []\n",
    "        self.wrong = []\n",
    "        self.encoded_sentences = []\n",
    "        self.encoded_correct = []\n",
    "        self.encoded_wrong = []\n",
    "\n",
    "        with open(nounpp_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.split()\n",
    "                sentence = line[1:7]\n",
    "                condition = \" \".join(line[7:9])\n",
    "                wrong = line[9]\n",
    "                correct = line[6]\n",
    "                \n",
    "                # Encode each word individually\n",
    "                encoded_sentence = [\n",
    "                    tokenizer.stoi.get(word.lower(), 0)  # Get token ID directly\n",
    "                    for word in sentence\n",
    "                ]\n",
    "                encoded_correct = tokenizer.stoi.get(correct.lower(), 0)\n",
    "                encoded_wrong = tokenizer.stoi.get(wrong.lower(), 0)\n",
    "                \n",
    "                self.sentences.append(sentence)\n",
    "                self.conditions.append(condition)\n",
    "                self.correct.append(correct)\n",
    "                self.wrong.append(wrong)\n",
    "                self.encoded_sentences.append(encoded_sentence)  # Now a list of ints\n",
    "                self.encoded_correct.append(encoded_correct)      # Now an int\n",
    "                self.encoded_wrong.append(encoded_wrong)          # Now an int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"sentence\": self.sentences[idx],\n",
    "            \"encoded_sentence\": torch.tensor(\n",
    "                self.encoded_sentences[idx], dtype=torch.long\n",
    "            ),  # [6] \n",
    "            \"correct\": self.correct[idx],\n",
    "            \"encoded_correct\": self.encoded_correct[idx],  # scalar int\n",
    "            \"wrong\": self.wrong[idx],\n",
    "            \"encoded_wrong\": self.encoded_wrong[idx],      # scalar int\n",
    "            \"condition\": self.conditions[idx],\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "def collate_fn_nounpp(batch):\n",
    "    sentences = [item[\"sentence\"] for item in batch]\n",
    "    encoded_sentences = torch.stack([item[\"encoded_sentence\"] for item in batch])\n",
    "    \n",
    "    # Convert lists of scalars to tensors\n",
    "    encoded_correct = torch.tensor([item[\"encoded_correct\"] for item in batch], dtype=torch.long)\n",
    "    encoded_wrong = torch.tensor([item[\"encoded_wrong\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    correct = [item[\"correct\"] for item in batch]\n",
    "    wrong = [item[\"wrong\"] for item in batch]\n",
    "    conditions = [item[\"condition\"] for item in batch]\n",
    "\n",
    "    return {\n",
    "        \"sentence\": sentences,\n",
    "        \"encoded_sentence\": encoded_sentences,  # [batch_size, 6]\n",
    "        \"correct\": correct,\n",
    "        \"encoded_correct\": encoded_correct,     # [batch_size]\n",
    "        \"wrong\": wrong,\n",
    "        \"encoded_wrong\": encoded_wrong,         # [batch_size]\n",
    "        \"condition\": conditions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62cb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer.load(\"data/tokenizer/tokenizer.json\")\n",
    "test_dataset = NounPPDataset('tests/test_datasets/nounpp.txt', tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000, collate_fn=collate_fn_nounpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/scratch2/mrenaudin/Hard-CBR-RNN/checkpoints/job_lstm_003/lightning_logs/version_1851441/checkpoints/epoch=49-step=309500.ckpt'\n",
    "model=SimpleLSTM_LM.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ef4c4",
   "metadata": {},
   "source": [
    "# Eval function 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_attention_analysis(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    Efficient evaluation: Process first 5 words to predict verb at position 5.\n",
    "    Works with both CBR-RNN and Transformer models.\n",
    "    \"\"\"\n",
    "    condition_accuracies = defaultdict(int)\n",
    "    condition_counts = defaultdict(int)\n",
    "    sentence_details = []\n",
    "    attention_by_condition = defaultdict(list) \n",
    "    \n",
    "    model.model.eval()\n",
    "    \n",
    "    # Detect model type\n",
    "    is_transformer = hasattr(model, 'token_embedding')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            written = batch[\"sentence\"]\n",
    "            sentence = batch[\"encoded_sentence\"]  \n",
    "            correct = batch[\"encoded_correct\"]\n",
    "            wrong = batch[\"encoded_wrong\"]\n",
    "            condition = batch[\"condition\"]\n",
    "            batch_size = sentence.size(0)\n",
    "            limit = sentence.size(1)-1\n",
    "            context = sentence[:, :limit]\n",
    "            hidden = model.model.init_hidden(context.size(0), device = 'cpu')\n",
    "            out,_=model.model(context,hidden)\n",
    "            log_probs = torch.nn.functional.log_softmax(out[:, -1, :], dim=-1)\n",
    "            correct_log_probs = log_probs[torch.arange(batch_size), correct]\n",
    "            wrong_log_probs = log_probs[torch.arange(batch_size), wrong]\n",
    "            correct_predictions = correct_log_probs >= wrong_log_probs\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                cond = condition[i]\n",
    "                pred = correct_predictions[i].item()\n",
    "                \n",
    "                condition_counts[cond] += 1\n",
    "                condition_accuracies[cond] += pred\n",
    "\n",
    "                sentence_details.append({\n",
    "                    \"sentence\": written[i],\n",
    "                    \"condition\": cond,\n",
    "                    \"correct_log_prob\": correct_log_probs[i].item(),\n",
    "                    \"wrong_log_prob\": wrong_log_probs[i].item(),\n",
    "                    \"model_prefers_correct\": pred,\n",
    "                })\n",
    "                \n",
    "\n",
    "    final_accuracies = {\n",
    "        cond: condition_accuracies[cond] / condition_counts[cond]\n",
    "        for cond in condition_accuracies\n",
    "    }\n",
    "    \n",
    "\n",
    "    return final_accuracies, None, sentence_details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = eval_with_attention_analysis(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71206d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f03be",
   "metadata": {},
   "source": [
    "# Eval function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sentence = \" \".join(\n",
    "    [\n",
    "        \"In service , the aircraft was operated by a crew of five and could accommodate either 30 paratroopers , 32 <unk> and 28 sitting casualties , or 50 fully equipped troops .\",\n",
    "        'He even speculated that technical classes might some day be held \" for the better training of workmen in their several crafts and industries .',\n",
    "        \"After the War of the Holy League in 1537 against the Ottoman Empire , a truce between Venice and the Ottomans was created in 1539 .\",\n",
    "        'Moore says : \" Tony and I had a good <unk> and off-screen relationship , we are two very different people , but we did share a sense of humour \" .',\n",
    "        \"<unk> is also the basis for online games sold through licensed lotteries .\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def feed_input(model, hidden, w):\n",
    "    if w not in tokenizer.stoi:\n",
    "        print(f\"Warning: '{w}' not in vocabulary, using <unk> token\")\n",
    "        w = '<unk>'\n",
    "    inp = torch.autograd.Variable(\n",
    "        torch.LongTensor([[tokenizer.stoi[w]]]).to('cpu')\n",
    "    )\n",
    "    out, hidden= model.model(inp, hidden)\n",
    "    return out, hidden\n",
    "\n",
    "\n",
    "def feed_sentence(model, h, sentence):\n",
    "    outs = []\n",
    "    for w in sentence:\n",
    "        out, h = feed_input(model, h, w)\n",
    "        outs.append(torch.nn.functional.log_softmax(out[0]).unsqueeze(0))\n",
    "    return outs, h \n",
    "\n",
    "\n",
    "# evaluation function\n",
    "def eval(model, test_dataloader, init_sentence):\n",
    "    condition_accuracies = defaultdict(int)\n",
    "    condition_counts = defaultdict(int)\n",
    "    correct_pred = 0\n",
    "    sentence_details = []\n",
    "\n",
    "    model.model.eval()\n",
    "\n",
    "    hidden = model.model.init_hidden(1, device='cpu')\n",
    "    init_out, init_h= feed_sentence(model, hidden, init_sentence.split(\" \"))\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            out = None\n",
    "            written = batch[\"sentence\"]\n",
    "            sentence = batch[\"encoded_sentence\"]\n",
    "            correct = batch[\"encoded_correct\"]\n",
    "            wrong = batch[\"encoded_wrong\"]\n",
    "            condition = batch[\"condition\"]\n",
    "            batch_size = sentence.size(0)\n",
    "            hidden = (\n",
    "                init_h[0].expand(-1, batch_size, -1).contiguous(),\n",
    "                init_h[1].expand(-1, batch_size, -1).contiguous(),\n",
    "            )\n",
    "\n",
    "            for w in range(sentence.shape[1] - 1):\n",
    "\n",
    "                word = torch.autograd.Variable(sentence[:, w:w+1])#.unsqueeze(0))\n",
    "                out, hidden = model.model(word, hidden)\n",
    "            print(out.shape)\n",
    "            log_probs = torch.nn.functional.log_softmax(out.squeeze(1), dim=-1)\n",
    "            correct_log_probs = log_probs[torch.arange(batch_size), correct]\n",
    "            wrong_log_probs = log_probs[torch.arange(batch_size), wrong]\n",
    "            correct_predictions = correct_log_probs >= wrong_log_probs\n",
    "            for i in range(batch_size):\n",
    "                cond = condition[i]\n",
    "                pred = correct_predictions[i].item()\n",
    "                condition_counts[cond] += 1\n",
    "                condition_accuracies[cond] += pred\n",
    "\n",
    "                sentence_details.append(\n",
    "                    {\n",
    "                        \"sentence\": written[i],\n",
    "                        \"condition\": condition[i],\n",
    "                        \"correct_log_prob\": correct_log_probs[i],\n",
    "                        \"wrong_log_prob\": wrong_log_probs[i],\n",
    "                        \"model_prefers_correct\": pred,\n",
    "                    }\n",
    "                )\n",
    "    final_accuracies = {\n",
    "        cond: condition_accuracies[cond] / condition_counts[cond]\n",
    "        for cond in condition_accuracies\n",
    "    }\n",
    "\n",
    "    return final_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = eval(model, test_dataloader, init_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b88776",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No init_sentence needed anymore!\n",
    "\n",
    "def eval_no_priming(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    Evaluation without priming - fresh hidden state for each batch\n",
    "    \"\"\"\n",
    "    condition_accuracies = defaultdict(int)\n",
    "    condition_counts = defaultdict(int)\n",
    "    sentence_details = []\n",
    "    model.model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            written = batch[\"sentence\"]\n",
    "            sentence = batch[\"encoded_sentence\"]\n",
    "            correct = batch[\"encoded_correct\"]\n",
    "            wrong = batch[\"encoded_wrong\"]\n",
    "            condition = batch[\"condition\"]\n",
    "            batch_size = sentence.size(0)\n",
    "            \n",
    "            # Initialize fresh hidden state for this batch (no priming!)\n",
    "            hidden = model.model.init_hidden(batch_size, device='cpu')\n",
    "            \n",
    "            # Process sentence word-by-word\n",
    "            for w in range(sentence.shape[1] - 1):\n",
    "                word = sentence[:, w].unsqueeze(1)  # [batch_size, 1]\n",
    "                out, hidden = model.model(word, hidden)\n",
    "            \n",
    "            # Get predictions from final output\n",
    "            log_probs = torch.nn.functional.log_softmax(out.squeeze(1), dim=-1)  # [batch_size, vocab_size]\n",
    "            correct_log_probs = log_probs[torch.arange(batch_size), correct]\n",
    "            wrong_log_probs = log_probs[torch.arange(batch_size), wrong]\n",
    "            correct_predictions = correct_log_probs >= wrong_log_probs\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                cond = condition[i]\n",
    "                pred = correct_predictions[i].item()\n",
    "                condition_counts[cond] += 1\n",
    "                condition_accuracies[cond] += pred\n",
    "                sentence_details.append({\n",
    "                    \"sentence\": written[i],\n",
    "                    \"condition\": cond,\n",
    "                    \"correct_log_prob\": correct_log_probs[i].item(),\n",
    "                    \"wrong_log_prob\": wrong_log_probs[i].item(),\n",
    "                    \"model_prefers_correct\": pred,\n",
    "                })\n",
    "    \n",
    "    final_accuracies = {\n",
    "        cond: condition_accuracies[cond] / condition_counts[cond]\n",
    "        for cond in condition_accuracies\n",
    "    }\n",
    "    \n",
    "    return final_accuracies, sentence_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = eval_no_priming(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a4bf7",
   "metadata": {},
   "source": [
    "-> so the difference in performance is only explained by priming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8971716",
   "metadata": {},
   "source": [
    "# Testing Transformers with priming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/job_transformer_2_003/lightning_logs/version_1851787/checkpoints/epoch=49-step=309500.ckpt'\n",
    "model=SimpleTransformerLM.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc86ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_transformer(model, test_dataloader, init_sentence):\n",
    "    \"\"\"\n",
    "    Evaluation with priming for Transformer model\n",
    "    \"\"\"\n",
    "    condition_accuracies = defaultdict(int)\n",
    "    condition_counts = defaultdict(int)\n",
    "    sentence_details = []\n",
    "    model.model.eval()\n",
    "    max_seq_len = model.model.pos_embedding.num_embeddings  # 64\n",
    "\n",
    "    # Tokenize and encode init_sentence once\n",
    "    init_tokens = init_sentence.split(\" \")\n",
    "    init_encoded = torch.LongTensor([[\n",
    "        tokenizer.stoi.get(w, tokenizer.stoi.get('<unk>', 0)) for w in init_tokens\n",
    "    ]])  # [1, init_len]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            written = batch[\"sentence\"]\n",
    "            sentence = batch[\"encoded_sentence\"]  # [batch_size, seq_len]\n",
    "            correct = batch[\"encoded_correct\"]\n",
    "            wrong = batch[\"encoded_wrong\"]\n",
    "            condition = batch[\"condition\"]\n",
    "            batch_size = sentence.size(0)\n",
    "            device = sentence.device\n",
    "            seq_len = sentence.size(1)\n",
    "            \n",
    "            # Move init_encoded to correct device\n",
    "            init_encoded = init_encoded.to(device)\n",
    "            max_init_len = max_seq_len - (seq_len - 1)\n",
    "            if init_encoded.size(1) > max_init_len:\n",
    "                # Take the last max_init_len tokens (most recent context)\n",
    "                init_encoded = init_encoded[:, -max_init_len:]\n",
    "            \n",
    "            # Expand init_encoded to batch size\n",
    "            init_batch = init_encoded.expand(batch_size, -1)  # [batch_size, init_len]\n",
    "            \n",
    "            # Concatenate init_sentence with test sentence\n",
    "            # This gives the model context before processing the test sentence\n",
    "            full_sequence = torch.cat([init_batch, sentence[:, :-1]], dim=1)  # [batch_size, init_len + seq_len - 1]\n",
    "            \n",
    "            # Forward pass through transformer\n",
    "            # Transpose for model: [batch_size, seq_len] -> [seq_len, batch_size]\n",
    "            out = model.model(full_sequence.transpose(0, 1))  # [seq_len, batch_size, vocab_size]\n",
    "            \n",
    "            # Get output at the position corresponding to the last word of the test sentence\n",
    "            # We want the prediction after seeing init_sentence + test_sentence[:-1]\n",
    "            pred_position = -1  # Last position\n",
    "            log_probs = torch.nn.functional.log_softmax(out[pred_position], dim=-1)  # [batch_size, vocab_size]\n",
    "            \n",
    "            correct_log_probs = log_probs[torch.arange(batch_size), correct]\n",
    "            wrong_log_probs = log_probs[torch.arange(batch_size), wrong]\n",
    "            correct_predictions = correct_log_probs >= wrong_log_probs\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                cond = condition[i]\n",
    "                pred = correct_predictions[i].item()\n",
    "                condition_counts[cond] += 1\n",
    "                condition_accuracies[cond] += pred\n",
    "                sentence_details.append({\n",
    "                    \"sentence\": written[i],\n",
    "                    \"condition\": cond,\n",
    "                    \"correct_log_prob\": correct_log_probs[i].item(),\n",
    "                    \"wrong_log_prob\": wrong_log_probs[i].item(),\n",
    "                    \"model_prefers_correct\": pred,\n",
    "                })\n",
    "    \n",
    "    final_accuracies = {\n",
    "        cond: condition_accuracies[cond] / condition_counts[cond]\n",
    "        for cond in condition_accuracies\n",
    "    }\n",
    "    \n",
    "    return final_accuracies, sentence_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26fe2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = WordTokenizer.load(\"tokenizer.json\")\n",
    "test_dataset = NounPPDataset('tests/test_datasets/nounpp.txt', tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000, collate_fn=collate_fn_nounpp)\n",
    "init_sentence = \" \".join(\n",
    "    [\n",
    "        \"In service , the aircraft was operated by a crew of five and could accommodate either 30 paratroopers , 32 <unk> and 28 sitting casualties , or 50 fully equipped troops .\",\n",
    "        'He even speculated that technical classes might some day be held \" for the better training of workmen in their several crafts and industries .',\n",
    "        \"After the War of the Holy League in 1537 against the Ottoman Empire , a truce between Venice and the Ottomans was created in 1539 .\",\n",
    "        'Moore says : \" Tony and I had a good <unk> and off-screen relationship , we are two very different people , but we did share a sense of humour \" .',\n",
    "        \"<unk> is also the basis for online games sold through licensed lotteries .\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dcdf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res = eval_transformer(model, test_dataloader, init_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481948a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229bc971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_transformer_simple(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    Simple evaluation for Transformer - no priming needed\n",
    "    Transformers don't benefit from priming the same way LSTMs do\n",
    "    \"\"\"\n",
    "    condition_accuracies = defaultdict(int)\n",
    "    condition_counts = defaultdict(int)\n",
    "    sentence_details = []\n",
    "    model.model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            written = batch[\"sentence\"]\n",
    "            sentence = batch[\"encoded_sentence\"]  # [batch_size, seq_len]\n",
    "            correct = batch[\"encoded_correct\"]\n",
    "            wrong = batch[\"encoded_wrong\"]\n",
    "            condition = batch[\"condition\"]\n",
    "            batch_size = sentence.size(0)\n",
    "            \n",
    "            # Process test sentence directly\n",
    "            context = sentence[:, :-1]  # [batch_size, seq_len-1]\n",
    "            \n",
    "            # Forward pass through transformer\n",
    "            out = model.model(context.transpose(0, 1))  # [seq_len-1, batch_size, vocab_size]\n",
    "            \n",
    "            # Get predictions from last position\n",
    "            log_probs = torch.nn.functional.log_softmax(out[-1], dim=-1)  # [batch_size, vocab_size]\n",
    "            \n",
    "            correct_log_probs = log_probs[torch.arange(batch_size), correct]\n",
    "            wrong_log_probs = log_probs[torch.arange(batch_size), wrong]\n",
    "            correct_predictions = correct_log_probs >= wrong_log_probs\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                cond = condition[i]\n",
    "                pred = correct_predictions[i].item()\n",
    "                condition_counts[cond] += 1\n",
    "                condition_accuracies[cond] += pred\n",
    "                sentence_details.append({\n",
    "                    \"sentence\": written[i],\n",
    "                    \"condition\": cond,\n",
    "                    \"correct_log_prob\": correct_log_probs[i].item(),\n",
    "                    \"wrong_log_prob\": wrong_log_probs[i].item(),\n",
    "                    \"model_prefers_correct\": pred,\n",
    "                })\n",
    "    \n",
    "    final_accuracies = {\n",
    "        cond: condition_accuracies[cond] / condition_counts[cond]\n",
    "        for cond in condition_accuracies\n",
    "    }\n",
    "    \n",
    "    return final_accuracies, sentence_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f750126",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = eval_transformer_simple(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fad9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b321b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the tensorboard log\n",
    "log_path = '/scratch2/mrenaudin/Hard-CBR-RNN/checkpoints/job_transformer_2_003/lightning_logs/version_1851787/events.out.tfevents.1758631281.jzxh004.4137051.0'\n",
    "\n",
    "ea = event_accumulator.EventAccumulator(log_path)\n",
    "ea.Reload()\n",
    "\n",
    "print(\"=== Available Tags ===\")\n",
    "print(\"Scalars:\", ea.Tags()['scalars'])\n",
    "print()\n",
    "\n",
    "# Extract key metrics\n",
    "def get_metric_data(ea, tag):\n",
    "    try:\n",
    "        events = ea.Scalars(tag)\n",
    "        steps = [e.step for e in events]\n",
    "        values = [e.value for e in events]\n",
    "        return steps, values\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# Get training and validation metrics\n",
    "metrics_to_check = [\n",
    "    'train_loss', 'train_ppl', 'train_accuracy',\n",
    "    'val_loss', 'val_ppl', 'val_accuracy',\n",
    "    'learning_rate', 'grad_norm',\n",
    "    'train_entropy_norm', 'val_entropy_norm',\n",
    "    'train_confidence', 'val_confidence',\n",
    "    'train_repetition_ratio', 'val_repetition_ratio'\n",
    "]\n",
    "\n",
    "print(\"=== Training Summary ===\\n\")\n",
    "for metric in metrics_to_check:\n",
    "    steps, values = get_metric_data(ea, metric)\n",
    "    if steps:\n",
    "        print(f\"{metric:25s}: Start={values[0]:.4f}, Final={values[-1]:.4f}, Best={min(values) if 'loss' in metric else max(values):.4f}, Steps={len(steps)}\")\n",
    "\n",
    "# Plot key metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Transformer Training Analysis', fontsize=16)\n",
    "\n",
    "# 1. Loss curves\n",
    "ax = axes[0, 0]\n",
    "steps, train_loss = get_metric_data(ea, 'train_loss')\n",
    "if steps:\n",
    "    ax.plot(steps, train_loss, label='Train Loss', alpha=0.7)\n",
    "steps, val_loss = get_metric_data(ea, 'val_loss')\n",
    "if steps:\n",
    "    ax.plot(steps, val_loss, label='Val Loss', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Perplexity\n",
    "ax = axes[0, 1]\n",
    "steps, train_ppl = get_metric_data(ea, 'train_ppl')\n",
    "if steps:\n",
    "    ax.plot(steps, train_ppl, label='Train PPL', alpha=0.7)\n",
    "steps, val_ppl = get_metric_data(ea, 'val_ppl')\n",
    "if steps:\n",
    "    ax.plot(steps, val_ppl, label='Val PPL', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.set_title('Perplexity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, min(200, max(train_ppl) if train_ppl else 200))\n",
    "\n",
    "# 3. Learning rate\n",
    "ax = axes[0, 2]\n",
    "steps, lr = get_metric_data(ea, 'learning_rate')\n",
    "if steps:\n",
    "    ax.plot(steps, lr)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Gradient norm\n",
    "ax = axes[1, 0]\n",
    "steps, grad_norm = get_metric_data(ea, 'grad_norm')\n",
    "if steps:\n",
    "    ax.plot(steps, grad_norm)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Gradient Norm')\n",
    "ax.set_title('Gradient Norm (Should be stable)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Entropy (prediction diversity)\n",
    "ax = axes[1, 1]\n",
    "steps, train_ent = get_metric_data(ea, 'train_entropy_norm')\n",
    "if steps:\n",
    "    ax.plot(steps, train_ent, label='Train', alpha=0.7)\n",
    "steps, val_ent = get_metric_data(ea, 'val_entropy_norm')\n",
    "if steps:\n",
    "    ax.plot(steps, val_ent, label='Val', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Normalized Entropy')\n",
    "ax.set_title('Prediction Diversity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0.1, color='r', linestyle='--', alpha=0.5, label='Low diversity threshold')\n",
    "\n",
    "# 6. Repetition ratio\n",
    "ax = axes[1, 2]\n",
    "steps, train_rep = get_metric_data(ea, 'train_repetition_ratio')\n",
    "if steps:\n",
    "    ax.plot(steps, train_rep, label='Train', alpha=0.7)\n",
    "steps, val_rep = get_metric_data(ea, 'val_repetition_ratio')\n",
    "if steps:\n",
    "    ax.plot(steps, val_rep, label='Val', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Repetition Ratio')\n",
    "ax.set_title('Model Collapse Indicator (Lower is better)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Collapse threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('transformer_training_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get final values\n",
    "_, final_train_ppl = get_metric_data(ea, 'train_ppl')\n",
    "_, final_val_ppl = get_metric_data(ea, 'val_ppl')\n",
    "_, final_entropy = get_metric_data(ea, 'val_entropy_norm')\n",
    "_, final_rep = get_metric_data(ea, 'val_repetition_ratio')\n",
    "\n",
    "if final_train_ppl:\n",
    "    print(f\"\\nFinal Training PPL: {final_train_ppl[-1]:.2f}\")\n",
    "    print(f\"Final Validation PPL: {final_val_ppl[-1]:.2f}\")\n",
    "    \n",
    "    if final_val_ppl[-1] > 50:\n",
    "        print(\"⚠️  HIGH PERPLEXITY - Model didn't train well!\")\n",
    "        print(\"   → Model is still very uncertain about predictions\")\n",
    "    elif final_val_ppl[-1] > 30:\n",
    "        print(\"⚠️  MODERATE PERPLEXITY - Training could be better\")\n",
    "    else:\n",
    "        print(\"✓ Good perplexity - Model trained reasonably\")\n",
    "\n",
    "if final_entropy:\n",
    "    print(f\"\\nFinal Validation Entropy: {final_entropy[-1]:.3f}\")\n",
    "    if final_entropy[-1] < 0.2:\n",
    "        print(\"⚠️  LOW ENTROPY - Model predictions are NOT diverse\")\n",
    "        print(\"   → Model might be collapsing to predict same tokens\")\n",
    "    else:\n",
    "        print(\"✓ Reasonable entropy\")\n",
    "\n",
    "if final_rep:\n",
    "    print(f\"\\nFinal Validation Repetition: {final_rep[-1]:.3f}\")\n",
    "    if final_rep[-1] > 0.3:\n",
    "        print(\"⚠️  HIGH REPETITION - Model is repeating same prediction\")\n",
    "        print(\"   → This explains the attention collapse!\")\n",
    "    else:\n",
    "        print(\"✓ Low repetition ratio\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45d830",
   "metadata": {},
   "source": [
    "# Test on CBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef4d28c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBR_LM(\n",
       "  (model): CBR_RNN(\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (encoder): Embedding(49999, 1024)\n",
       "    (q): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (intermediate_h): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (final_h): Linear(in_features=4096, out_features=3072, bias=True)\n",
       "    (decoder): Linear(in_features=1024, out_features=49999, bias=True)\n",
       "    (q_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (int_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (f_norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = '/scratch2/mrenaudin/Hard-CBR-RNN/checkpoints/job_cbr_2_001/lightning_logs/version_1850825/checkpoints/epoch=49-step=309500.ckpt'\n",
    "model=CBR_LM.load_from_checkpoint(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d312a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sentence = \" \".join(\n",
    "    [\n",
    "        \"In service , the aircraft was operated by a crew of five and could accommodate either 30 paratroopers , 32 <unk> and 28 sitting casualties , or 50 fully equipped troops .\",\n",
    "        'He even speculated that technical classes might some day be held \" for the better training of workmen in their several crafts and industries .',\n",
    "        \"After the War of the Holy League in 1537 against the Ottoman Empire , a truce between Venice and the Ottomans was created in 1539 .\",\n",
    "        'Moore says : \" Tony and I had a good <unk> and off-screen relationship , we are two very different people , but we did share a sense of humour \" .',\n",
    "        \"<unk> is also the basis for online games sold through licensed lotteries .\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "975bc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cbr_batched(model, test_dataloader, init_sentence):\n",
    "    \"\"\"\n",
    "    More efficient batched evaluation for CBR-RNN with priming\n",
    "    \"\"\"\n",
    "    condition_accuracies = defaultdict(int)\n",
    "    condition_counts = defaultdict(int)\n",
    "    sentence_details = []\n",
    "    model.model.eval()\n",
    "    \n",
    "    # Tokenize and encode init_sentence\n",
    "    init_tokens = init_sentence.split(\" \")\n",
    "    init_encoded = torch.LongTensor([\n",
    "        [tokenizer.stoi.get(w, tokenizer.stoi.get('<unk>', 0)) for w in init_tokens]\n",
    "    ]).transpose(0, 1)  # [init_len, 1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            written = batch[\"sentence\"]\n",
    "            sentence = batch[\"encoded_sentence\"]  # [batch_size, seq_len]\n",
    "            correct = batch[\"encoded_correct\"]\n",
    "            wrong = batch[\"encoded_wrong\"]\n",
    "            condition = batch[\"condition\"]\n",
    "            batch_size = sentence.size(0)\n",
    "            device = sentence.device\n",
    "            \n",
    "            # Move init_encoded to device\n",
    "            init_encoded_device = init_encoded.to(device)\n",
    "            \n",
    "            # Process priming sentence with batch_size=1\n",
    "            cache = model.model.init_cache(sentence.transpose(0,1))\n",
    "            # _, primed_states = model.model(init_encoded_device, init_cache)\n",
    "            \n",
    "            # # Extract final primed cache\n",
    "            # primed_hidden = primed_states[-1:, :, :]  # [1, 1, nhid]\n",
    "            \n",
    "            # # Expand to batch size\n",
    "            # batch_cache = tuple(\n",
    "            #     primed_hidden.expand(-1, batch_size, -1).contiguous()\n",
    "            #     for _ in range(3)  # (hidden, keys, values)\n",
    "            # )\n",
    "            \n",
    "            # Process test sequence (excluding last token)\n",
    "            test_context = sentence[:, :-1].transpose(0, 1)  # [seq_len-1, batch_size]\n",
    "            out, _ = model.model(test_context, cache)\n",
    "            \n",
    "            # Get predictions from last position\n",
    "            # out shape: [seq_len-1, batch_size, vocab_size]\n",
    "            log_probs = torch.nn.functional.log_softmax(out[-1], dim=-1)  # [batch_size, vocab_size]\n",
    "            \n",
    "            correct_log_probs = log_probs[torch.arange(batch_size), correct]\n",
    "            wrong_log_probs = log_probs[torch.arange(batch_size), wrong]\n",
    "            correct_predictions = correct_log_probs >= wrong_log_probs\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                cond = condition[i]\n",
    "                pred = correct_predictions[i].item()\n",
    "                condition_counts[cond] += 1\n",
    "                condition_accuracies[cond] += pred\n",
    "                sentence_details.append({\n",
    "                    \"sentence\": written[i],\n",
    "                    \"condition\": cond,\n",
    "                    \"correct_log_prob\": correct_log_probs[i].item(),\n",
    "                    \"wrong_log_prob\": wrong_log_probs[i].item(),\n",
    "                    \"model_prefers_correct\": pred,\n",
    "                })\n",
    "    \n",
    "    final_accuracies = {\n",
    "        cond: condition_accuracies[cond] / condition_counts[cond]\n",
    "        for cond in condition_accuracies\n",
    "    }\n",
    "    \n",
    "    return final_accuracies, sentence_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dbaa97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, sent = eval_cbr_batched(model, test_dataloader, init_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83b3a06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'singular singular': 0.295,\n",
       " 'singular plural': 0.313,\n",
       " 'plural singular': 0.696,\n",
       " 'plural plural': 0.707}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b012336d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'valid_data.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid_data.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/leaps3/lib/python3.9/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/leaps3/lib/python3.9/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/leaps3/lib/python3.9/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'valid_data.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "valid = torch.load('valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244b0775",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grid_search.data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrid_search\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m WordTokenizer\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grid_search.data_utils'"
     ]
    }
   ],
   "source": [
    "from grid_search.data_utils import WordTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = WordTokenizer.load(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0e94d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(valid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a93ae42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grid_search.data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgrid_search\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load tokenizer and data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m WordTokenizer\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grid_search.data_utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from grid_search.data_utils import WordTokenizer\n",
    "\n",
    "# Load tokenizer and data\n",
    "tokenizer = WordTokenizer.load(\"tokenizer.json\")\n",
    "train_data = torch.load('train_data.pt')\n",
    "\n",
    "# Count unknown tokens\n",
    "total_tokens = len(train_data)\n",
    "unk_tokens = (train_data == tokenizer.unk_id).sum().item()\n",
    "\n",
    "# Calculate percentage\n",
    "unk_percentage = (unk_tokens / total_tokens) * 100\n",
    "\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Unknown tokens: {unk_tokens:,}\")\n",
    "print(f\"Unknown percentage: {unk_percentage:.2f}%\")\n",
    "\n",
    "# Also show vocabulary coverage\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"\\nVocabulary size: {vocab_size:,}\")\n",
    "print(f\"Tokens used: {len(torch.unique(train_data)):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c7353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaps3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
